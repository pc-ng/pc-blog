{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Pc's Blog \u2693\ufe0e It is alright to be imperfect. .welcome { padding-left: .1em; margin-bottom: 0 } .site-name { margin-bottom: .5em !important; color: blue !important; } .site-description { font-size: large; padding-left: .05em; margin-bottom: 0; } .md-typeset .md-button { font-size: unset; min-width: 3em; text-align: center; padding: .3em 0 0 0; border-radius: .5em; border: 1px solid lightgray; color: unset; }","title":"Home"},{"location":"#pcs-blog","text":"It is alright to be imperfect. .welcome { padding-left: .1em; margin-bottom: 0 } .site-name { margin-bottom: .5em !important; color: blue !important; } .site-description { font-size: large; padding-left: .05em; margin-bottom: 0; } .md-typeset .md-button { font-size: unset; min-width: 3em; text-align: center; padding: .3em 0 0 0; border-radius: .5em; border: 1px solid lightgray; color: unset; }","title":"Pc's Blog"},{"location":"artificial_intelligence/","text":"-- ..","title":"Deep Learning"},{"location":"artificial_intelligence/timeseries/dtw/","text":"Dynamic Time Warping (DTW) is a non-linear similarity computation method that dynamically compute the similarity between time series data when the time indices between data points from time series A and time series B do not match. Consider the two time series sequences (time series A in green, and time series B in blue) shown in Figure below, these two sequences do not line up in time axis. However, both of them have some similarities in terms of their component shapes. If traditional distance metrics (e.g., Euclidean distance, Manhattan distance, etc.) are used to compute the distance between the \\(i\\) th point of time series A with the \\(i\\) th point of time series B, it will most probably produce a poor similarity score. If a non-linear mapping can be used to match the similar shape of two time series even though these two time series sequences are out of phase in the time axis, it will produce a more intuitive similarity measure. This can be done by warping the time axis of one sequences to align the time axis. DTW can efficiently align two time series sequences, allowing a more intuitive similarity measure between out of sync data points. Warping Function \u2693\ufe0e DTW uses warping function to find the best alignment between two time series sequences. The objective is to find the path through the grids \\[ P = P_1, P_2, ..., P_s, ..., P_k \\] \\[ P_s = (i, j) \\] which minimizes the total distance between them. DTW aims to learn a warping path that dynamically maps the data points of time series A to data points of time series B. Let \\(A\\) and \\(B\\) be time series A and B, respectively, we can compute the time normalized distance between these two time series: \\[ D(A, B) = \\frac{\\sum_{s=1}^k d(P_s) w_s}{\\sum_{s=1}^k w_s} \\] where \\(d(P_s) = d(v_i, v_j)\\) is the distance between value at \\(i\\) th point \\(v_i\\) and value at \\(j\\) th point \\(v_j\\) , and \\(w_s\\) is the weight coefficient. Considering the Figure above, there are many possible warping paths through the grid. That's being said, to search for an optimum path, i.e., \\[ P_o = arg\\min_P (D(A, B)) \\] can be extremely hard when the grid size is big. Furthermore, if a single point of time series A can map onto a large subsection of time series B, it will lead to an unintuitive alignment. Over the decade, a few constraints have been imposed on the warping function: 1. Monotonicity \u2693\ufe0e Monotonicity ensures that the warping path does not go back in time. Given \\(P_s = (i, j)\\) and \\(P_{s-1} = (i', j')\\) , \\(i \\leq i'\\) and \\(j \\leq j'\\) forces the points in \\(P\\) to be monotonically spaced in time 2. Continuity \u2693\ufe0e Continuity ensures that the warping path does not jump in time. Given \\(P_s = (i, j)\\) and \\(P_{s-1} = (i', j')\\) , \\(i - i' \\leq 1\\) and \\(j - j' \\leq 1\\) restricts the allowable steps in the warping path to adjacent cells. 3. Boundary Conditions \u2693\ufe0e The boundary of the warping path states that the part should start at \\(P_1 = (1, 1)\\) and end at \\(P_s = (m, n)\\) , i.e., the warping path needs to start and finish in diagonally opposite corner of the grid. This is important to make sure the warping path does not consider only partial of the sequence. 4. Warping Window \u2693\ufe0e Let \\(r>0\\) be the length of warping window, \\(\\|i - j\\| \\leq r\\) restricts allowable grid points for the warping path. The warping window ensures that the warping path does not wander too far away from the diagonal. This guarantees that the alignment will not get stuck at similar features. 5. Slope Constraint \u2693\ufe0e Slope constraint ensures that the warping path is neither too steep or too shallow. Let \\(q\\) and \\(p\\) be the number of steps in the x-direction and y-direction given the grid, then \\(\\frac{j_{p} - j_{0}}{i_{p} - i_{0}} \\leq p\\) and \\(\\frac{i_{q} - i_{0}}{j_{q} - j_{0}} \\leq q\\) . That is, after \\(q\\) steps in x one must step in y and vice versa. Dynamic Programming \u2693\ufe0e Dynamic Programming is an efficient method to find the warping path. In general, dynamic programming evaluate the cumulative distance \\(\\gamma(i, j)\\) based on the distance \\(d(P_s)\\) at the current cell and the minimum of the cumulative distances of the adjacent elements. Mathematically, \\[ \\gamma(i, j) = d(P_s) + \\min \\{\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\} \\] where \\(d(P_s) = d(v_i, v_j)\\) can be calculated by taking the absolute difference between value at \\(i\\) th point and value at \\(j\\) th point, i.e., \\(d(v_i, v_j) = |v_i - v_j|\\) Example \u2693\ufe0e Suppose that we have two time series A and B as follows: \\[ A = [ 3, 2, 2, 3, 5, 5, 6 ] \\] \\[ B = [ 1, 3, 2, 2, 3, 5 ] \\] First, let's consider point \\(P_s = (1,1)\\) , i.e., \\(i=1\\) and \\(j=1\\) , the value at \\(i=1\\) is \\(v(i=1) = 3\\) and the value at \\(j=1\\) is \\(v(j=1) = 1\\) . Hence, the absolute distance at point \\(P_s = (1,1)\\) is \\(d(P_s) = |3-1| = 2\\) . Since the values at the cell above, left and diagonally above \\(P_s\\) are all empty, hence, \\(\\min \\{\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\} = 0\\) . The cumulative distance \\(\\gamma(i=1, j=1)\\) will be 2, as shown in Figure below. Let's consider another point \\(P_s = (4,4)\\) , here we have \\(v(i=4) = 3\\) and \\(v(j=4) = 2\\) . Hence, the absolute distance at point \\(P_s = (4,4)\\) is \\(d(P_s) = |3-2| = 1\\) . The values at the adjacent cells (above, left and diagonally above ) are 3, 2 and 2. By taking the minimum value, we have 2. So, the cumulative distance \\(\\gamma(i=4, j=4)\\) will be 3, as shown in Figure below. Figure below shows the final grid (or matrix) with all the value computed. The warping path can be obtained by backtracking from the end point to the beginning point as shown below. Implementation \u2693\ufe0e Here we will walkthrough a simple implementation of above example in Python. First, define the two time series A and B A = np . array ([ 3 , 2 , 2 , 3 , 5 , 5 , 6 ]) B = np . array ([ 1 , 3 , 2 , 2 , 3 , 5 ]) Initialize a dtw matrix based on the length of A and B, i.e., length of B define the number of rows, and length of A define the number of columns. dtw_mat = np . zeros (( len ( B ), len ( A ))) print ( dtw_mat ) Output: [[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.]] Now, we need to loop through all element in the dtw matrix, and compute the cumulative distance. # define the absolute distance function d = lambda x , y : np . abs ( x - y ) for i in range ( len ( B )): for j in range ( len ( A )): if i == 0 and j == 0 : dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) else : if i == 0 and j > 0 : choice = dtw_mat [ i , j - 1 ] elif i > 0 and j == 0 : choice = dtw_mat [ i - 1 , j ] else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) + np . min ( choice ) print ( dtw_mat ) Output: [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] We can find the warping path by backtracking. path = [[ len ( B ) - 1 , len ( A ) - 1 ]] while ( True ): print ( path ) i , j = path [ - 1 ][ 0 ], path [ - 1 ][ 1 ] if i == 0 and j == 0 : break elif i == 0 and j > 0 : path . append ([ i , j - 1 ]) elif i > 0 and j == 0 : path . append ([ i - 1 , j ]) else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] ind = [[ i - 1 , j ], [ i , j - 1 ], [ i - 1 , j - 1 ]] k = np . argmin ( choice ) path . append ( ind [ k ]) warp = np . zeros (( len ( B ), len ( A ))) for p in path : warp [ p [ 0 ], p [ 1 ]] = 1 print ( warp ) Output: [[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1.]] Then, we can calculate the normalized distance between time series A and time series B. Assume that all the point shares equal weight, i.e., \\(w_s=1\\) for all \\(P_s\\) , then \\(D(A,B)\\) can be computed as follows: \\[ D(A,B) = \\frac{1}{k} \\sum_{s = 1}^k d(P_s) \\] D = np . sum ( warp * dtw_mat ) / len ( path ) print ( f 'Normalized Distance: { D : 2f } ' ) output: Normalized Distance: 2.111111 Wrap Up the Implementation with function \u2693\ufe0e We can also define a few functions to handle the DTW matrix computation, get the warping path and also compute the normalized distance. def computed_dtwMat ( A , B ): d = lambda x , y : np . abs ( x - y ) for i in range ( len ( B )): for j in range ( len ( A )): if i == 0 and j == 0 : dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) else : if i == 0 and j > 0 : choice = dtw_mat [ i , j - 1 ] elif i > 0 and j == 0 : choice = dtw_mat [ i - 1 , j ] else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) + np . min ( choice ) return dtw_mat We can call the above function by providing time series A and time series B as input arguments, and it will return the dtw_mat . dtw_mat = computed_dtwMat ( A , B ) print ( dtw_mat ) Output: [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] We can define a function to get the warping path. def get_warpingPath ( A , B ): dtw_mat = computed_dtwMat ( A , B ) path = [[ len ( B ) - 1 , len ( A ) - 1 ]] while ( True ): i , j = path [ - 1 ][ 0 ], path [ - 1 ][ 1 ] if i == 0 and j == 0 : break elif i == 0 and j > 0 : path . append ([ i , j - 1 ]) elif i > 0 and j == 0 : path . append ([ i - 1 , j ]) else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] ind = [[ i - 1 , j ], [ i , j - 1 ], [ i - 1 , j - 1 ]] k = np . argmin ( choice ) path . append ( ind [ k ]) warp = np . zeros (( len ( B ), len ( A ))) for p in path : warp [ p [ 0 ], p [ 1 ]] = 1 ``` python path , warp , dtw_matrix = get_warpingPath ( A , B ) print ( warp ) print ( dtw_mat ) Output: [[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1.]] [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] Finally, we can have a function to compute the normalized distance. def normalized_dist ( A , B ): path , warp , dtw_matrix = get_warpingPath ( A , B ) D = np . sum ( warp * dtw_mat ) / len ( path ) return D By calling the function normalized_dist , it will return us the normalized distance between time series A and time series B. D = normalized_dist ( A , B ) print ( f 'Normalized Distance: { D : 2f } ' ) Output: Normalized Distance: 2.111111 Summary \u2693\ufe0e A python script containing all the above functions is provided here You can also refer to the example notebook here on how to apply those functions. There is also a useful dtw-python package provided by Toni dot Giorgino. You can visit his site for more information about dtw-python package. You can also install the package by issuing the following command: pip install dtw-python . DTW is available in Matlab through the Signal Processing Toolbox. For more information about using DTW with Matlab, check out the Matlab documentation about DTW here .","title":"Dynamic Time Warping"},{"location":"artificial_intelligence/timeseries/dtw/#warping-function","text":"DTW uses warping function to find the best alignment between two time series sequences. The objective is to find the path through the grids \\[ P = P_1, P_2, ..., P_s, ..., P_k \\] \\[ P_s = (i, j) \\] which minimizes the total distance between them. DTW aims to learn a warping path that dynamically maps the data points of time series A to data points of time series B. Let \\(A\\) and \\(B\\) be time series A and B, respectively, we can compute the time normalized distance between these two time series: \\[ D(A, B) = \\frac{\\sum_{s=1}^k d(P_s) w_s}{\\sum_{s=1}^k w_s} \\] where \\(d(P_s) = d(v_i, v_j)\\) is the distance between value at \\(i\\) th point \\(v_i\\) and value at \\(j\\) th point \\(v_j\\) , and \\(w_s\\) is the weight coefficient. Considering the Figure above, there are many possible warping paths through the grid. That's being said, to search for an optimum path, i.e., \\[ P_o = arg\\min_P (D(A, B)) \\] can be extremely hard when the grid size is big. Furthermore, if a single point of time series A can map onto a large subsection of time series B, it will lead to an unintuitive alignment. Over the decade, a few constraints have been imposed on the warping function:","title":"Warping Function"},{"location":"artificial_intelligence/timeseries/dtw/#1-monotonicity","text":"Monotonicity ensures that the warping path does not go back in time. Given \\(P_s = (i, j)\\) and \\(P_{s-1} = (i', j')\\) , \\(i \\leq i'\\) and \\(j \\leq j'\\) forces the points in \\(P\\) to be monotonically spaced in time","title":"1. Monotonicity"},{"location":"artificial_intelligence/timeseries/dtw/#2-continuity","text":"Continuity ensures that the warping path does not jump in time. Given \\(P_s = (i, j)\\) and \\(P_{s-1} = (i', j')\\) , \\(i - i' \\leq 1\\) and \\(j - j' \\leq 1\\) restricts the allowable steps in the warping path to adjacent cells.","title":"2. Continuity"},{"location":"artificial_intelligence/timeseries/dtw/#3-boundary-conditions","text":"The boundary of the warping path states that the part should start at \\(P_1 = (1, 1)\\) and end at \\(P_s = (m, n)\\) , i.e., the warping path needs to start and finish in diagonally opposite corner of the grid. This is important to make sure the warping path does not consider only partial of the sequence.","title":"3. Boundary Conditions"},{"location":"artificial_intelligence/timeseries/dtw/#4-warping-window","text":"Let \\(r>0\\) be the length of warping window, \\(\\|i - j\\| \\leq r\\) restricts allowable grid points for the warping path. The warping window ensures that the warping path does not wander too far away from the diagonal. This guarantees that the alignment will not get stuck at similar features.","title":"4. Warping Window"},{"location":"artificial_intelligence/timeseries/dtw/#5-slope-constraint","text":"Slope constraint ensures that the warping path is neither too steep or too shallow. Let \\(q\\) and \\(p\\) be the number of steps in the x-direction and y-direction given the grid, then \\(\\frac{j_{p} - j_{0}}{i_{p} - i_{0}} \\leq p\\) and \\(\\frac{i_{q} - i_{0}}{j_{q} - j_{0}} \\leq q\\) . That is, after \\(q\\) steps in x one must step in y and vice versa.","title":"5. Slope Constraint"},{"location":"artificial_intelligence/timeseries/dtw/#dynamic-programming","text":"Dynamic Programming is an efficient method to find the warping path. In general, dynamic programming evaluate the cumulative distance \\(\\gamma(i, j)\\) based on the distance \\(d(P_s)\\) at the current cell and the minimum of the cumulative distances of the adjacent elements. Mathematically, \\[ \\gamma(i, j) = d(P_s) + \\min \\{\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\} \\] where \\(d(P_s) = d(v_i, v_j)\\) can be calculated by taking the absolute difference between value at \\(i\\) th point and value at \\(j\\) th point, i.e., \\(d(v_i, v_j) = |v_i - v_j|\\)","title":"Dynamic Programming"},{"location":"artificial_intelligence/timeseries/dtw/#example","text":"Suppose that we have two time series A and B as follows: \\[ A = [ 3, 2, 2, 3, 5, 5, 6 ] \\] \\[ B = [ 1, 3, 2, 2, 3, 5 ] \\] First, let's consider point \\(P_s = (1,1)\\) , i.e., \\(i=1\\) and \\(j=1\\) , the value at \\(i=1\\) is \\(v(i=1) = 3\\) and the value at \\(j=1\\) is \\(v(j=1) = 1\\) . Hence, the absolute distance at point \\(P_s = (1,1)\\) is \\(d(P_s) = |3-1| = 2\\) . Since the values at the cell above, left and diagonally above \\(P_s\\) are all empty, hence, \\(\\min \\{\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\} = 0\\) . The cumulative distance \\(\\gamma(i=1, j=1)\\) will be 2, as shown in Figure below. Let's consider another point \\(P_s = (4,4)\\) , here we have \\(v(i=4) = 3\\) and \\(v(j=4) = 2\\) . Hence, the absolute distance at point \\(P_s = (4,4)\\) is \\(d(P_s) = |3-2| = 1\\) . The values at the adjacent cells (above, left and diagonally above ) are 3, 2 and 2. By taking the minimum value, we have 2. So, the cumulative distance \\(\\gamma(i=4, j=4)\\) will be 3, as shown in Figure below. Figure below shows the final grid (or matrix) with all the value computed. The warping path can be obtained by backtracking from the end point to the beginning point as shown below.","title":"Example"},{"location":"artificial_intelligence/timeseries/dtw/#implementation","text":"Here we will walkthrough a simple implementation of above example in Python. First, define the two time series A and B A = np . array ([ 3 , 2 , 2 , 3 , 5 , 5 , 6 ]) B = np . array ([ 1 , 3 , 2 , 2 , 3 , 5 ]) Initialize a dtw matrix based on the length of A and B, i.e., length of B define the number of rows, and length of A define the number of columns. dtw_mat = np . zeros (( len ( B ), len ( A ))) print ( dtw_mat ) Output: [[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.]] Now, we need to loop through all element in the dtw matrix, and compute the cumulative distance. # define the absolute distance function d = lambda x , y : np . abs ( x - y ) for i in range ( len ( B )): for j in range ( len ( A )): if i == 0 and j == 0 : dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) else : if i == 0 and j > 0 : choice = dtw_mat [ i , j - 1 ] elif i > 0 and j == 0 : choice = dtw_mat [ i - 1 , j ] else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) + np . min ( choice ) print ( dtw_mat ) Output: [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] We can find the warping path by backtracking. path = [[ len ( B ) - 1 , len ( A ) - 1 ]] while ( True ): print ( path ) i , j = path [ - 1 ][ 0 ], path [ - 1 ][ 1 ] if i == 0 and j == 0 : break elif i == 0 and j > 0 : path . append ([ i , j - 1 ]) elif i > 0 and j == 0 : path . append ([ i - 1 , j ]) else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] ind = [[ i - 1 , j ], [ i , j - 1 ], [ i - 1 , j - 1 ]] k = np . argmin ( choice ) path . append ( ind [ k ]) warp = np . zeros (( len ( B ), len ( A ))) for p in path : warp [ p [ 0 ], p [ 1 ]] = 1 print ( warp ) Output: [[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1.]] Then, we can calculate the normalized distance between time series A and time series B. Assume that all the point shares equal weight, i.e., \\(w_s=1\\) for all \\(P_s\\) , then \\(D(A,B)\\) can be computed as follows: \\[ D(A,B) = \\frac{1}{k} \\sum_{s = 1}^k d(P_s) \\] D = np . sum ( warp * dtw_mat ) / len ( path ) print ( f 'Normalized Distance: { D : 2f } ' ) output: Normalized Distance: 2.111111","title":"Implementation"},{"location":"artificial_intelligence/timeseries/dtw/#wrap-up-the-implementation-with-function","text":"We can also define a few functions to handle the DTW matrix computation, get the warping path and also compute the normalized distance. def computed_dtwMat ( A , B ): d = lambda x , y : np . abs ( x - y ) for i in range ( len ( B )): for j in range ( len ( A )): if i == 0 and j == 0 : dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) else : if i == 0 and j > 0 : choice = dtw_mat [ i , j - 1 ] elif i > 0 and j == 0 : choice = dtw_mat [ i - 1 , j ] else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) + np . min ( choice ) return dtw_mat We can call the above function by providing time series A and time series B as input arguments, and it will return the dtw_mat . dtw_mat = computed_dtwMat ( A , B ) print ( dtw_mat ) Output: [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] We can define a function to get the warping path. def get_warpingPath ( A , B ): dtw_mat = computed_dtwMat ( A , B ) path = [[ len ( B ) - 1 , len ( A ) - 1 ]] while ( True ): i , j = path [ - 1 ][ 0 ], path [ - 1 ][ 1 ] if i == 0 and j == 0 : break elif i == 0 and j > 0 : path . append ([ i , j - 1 ]) elif i > 0 and j == 0 : path . append ([ i - 1 , j ]) else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] ind = [[ i - 1 , j ], [ i , j - 1 ], [ i - 1 , j - 1 ]] k = np . argmin ( choice ) path . append ( ind [ k ]) warp = np . zeros (( len ( B ), len ( A ))) for p in path : warp [ p [ 0 ], p [ 1 ]] = 1 ``` python path , warp , dtw_matrix = get_warpingPath ( A , B ) print ( warp ) print ( dtw_mat ) Output: [[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1.]] [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] Finally, we can have a function to compute the normalized distance. def normalized_dist ( A , B ): path , warp , dtw_matrix = get_warpingPath ( A , B ) D = np . sum ( warp * dtw_mat ) / len ( path ) return D By calling the function normalized_dist , it will return us the normalized distance between time series A and time series B. D = normalized_dist ( A , B ) print ( f 'Normalized Distance: { D : 2f } ' ) Output: Normalized Distance: 2.111111","title":"Wrap Up the Implementation with function"},{"location":"artificial_intelligence/timeseries/dtw/#summary","text":"A python script containing all the above functions is provided here You can also refer to the example notebook here on how to apply those functions. There is also a useful dtw-python package provided by Toni dot Giorgino. You can visit his site for more information about dtw-python package. You can also install the package by issuing the following command: pip install dtw-python . DTW is available in Matlab through the Signal Processing Toolbox. For more information about using DTW with Matlab, check out the Matlab documentation about DTW here .","title":"Summary"},{"location":"biometric_signals/aboutecg/aboutecg/","text":"Electrocardiogram (ECG) \u2693\ufe0e Electrocardiogram (ECG) measures the electrical activity in our heart. ECG machine is commonly used in hospital to monitor patient's heart rate. Nowadays, a few consumer-grade device has integrated the ECG sensor into their smartwatch, for example Apple Watch and Fitbit . ECG Morphology \u2693\ufe0e ECG morphology can be described by 5 waves: P-wave, Q-wave, R-wave, S-wave, and T-wave. P-wave reflects atrial depolarization. It is a small wave due to the small muscle mass made by atria. T-wave reflects the rapid repolarization of contractile cell. Normal T-wave is slightly asymmetric, with a steeper downward slope. Q-wave, R-wave, S-wave are knowns as QRS complex in general. The QRS complex represents the depolarization of the ventricles. A QRS complex with a short duration implies a rapid depolarization, proving that the conduction system functions properly. A QRS complex with a long duration implies a slow depolarization, showing a high likelihood the dysfunction in conduction system. Determine the heart rate from ECG \u2693\ufe0e The heart rate can be determined by measuring the duration of RR interval between two successive QRS complexes. Once the RR interval is known, the heart rate can be computed as follows: \\[ HR = 60 \\frac{1}{d_{RR}} \\] where HR is heart rate is bpm, and \\(d_{RR}\\) is the duration of RR interval. Hence, the first problem to heart rate estimation is to locate the position of R-peak. R-peak Detection \u2693\ufe0e We will use a sample ECG data to demonstrate the R-peak detection, estimating the RR interval, and calculating the heart rate. This sample ECG data is generated artificially using neurokit2 . A 5 second data is generated with 100Hz sampling rate. The data is saved in csv file so we can use pandas to read the data. We can visualize the sample data using matplotlib. import pandas as pd import matplotlib.pyplot as plt import numpy as np fs = 100 data = pd . read_csv ( 'sample-ecg-data-70.csv' ) plt . figure ( figsize = ( 12 , 3 )) plt . plot ( data . index * ( 1 / fs ), data . ECG ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . show () Output: . From the plot, we can clearly identify the P-wave, T-wave, and the QRS complex. The goal is to locate all the R-peaks given the 5 second ECG signal. There are a number of ways to R-peaks detection, one of them is described in Halmiton algorithm Here, we will use a simple moving average approach to R-peak detection. The key steps of this approach is summarized as follows: compute the moving average with a window that is at least half the sampling frequency mark the region of interest (RoI) for those signals above the moving average locate the peak value and peak location for all RoIs calculate the average peak value of all peaks, and reject those peaks with peak value less than the average compute the duration between each RR-interval, and use the mean duration to compute the heart rate. Step 1: Moving average \u2693\ufe0e The sampling rate is 100Hz, so half the sampling rate is 50Hz, which means 50 data points per one second. # 1. compute the moving average with window of 50 data points window = 50 # use the convolve function by numpy to compute the moving average data [ 'ECG_ma' ] = np . convolve ( data . ECG , np . ones ( window ), 'same' ) / window plt . figure ( figsize = ( 12 , 3 )) plt . plot ( data . index * ( 1 / fs ), data . ECG , alpha = 0.5 , label = 'raw ECG data' ) plt . plot ( data . index * ( 1 / fs ), data . ECG_ma , label = 'moving average' ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . legend ( framealpha = 0.6 ) plt . show () Output: . Step 2: RoIs Identification \u2693\ufe0e From the above plot, we can see that some data points are above the moving average line, and some are below. We can mark those above the moving average line as RoI. # 2. find all the RoIs data [ 'roi' ] = np . zeros ( len ( data )) track_start = True for index , row in data . iterrows (): # check if the point is above the moving average point if ( row . ECG > row . ECG_ma ): if ( track_start ): k = int ( np . max ( data . roi ) + 1 ) print ( f '[ { k } ] RoI detected...' ) track_start = False data . roi . iloc [ index ] = k else : track_start = True Output: [1] RoI detected... [2] RoI detected... [3] RoI detected... [4] RoI detected... [5] RoI detected... [6] RoI detected... [7] RoI detected... [8] RoI detected... [9] RoI detected... [10] RoI detected... [11] RoI detected... [12] RoI detected... [13] RoI detected... [14] RoI detected... [15] RoI detected... [16] RoI detected... [17] RoI detected... [18] RoI detected... Step 3: Peak Detection \u2693\ufe0e For each RoI, we can compute the peak value and peak position by taking the maximum value. # 3. locate the peak for all RoIs groups = data . groupby ([ 'roi' ]) columns = [ 'roi' , 'value' , 'position' ] peak = pd . DataFrame ( columns = columns ) for k ,( roi , group ) in enumerate ( groups ): if ( roi != 0 ): peak = peak . append ( pd . Series ({ 'roi' : roi , 'value' : np . max ( group . ECG ), 'position' : group . index [ np . argmax ( group . ECG )] }, name = k - 1 )) plt . figure ( figsize = ( 12 , 3 )) plt . plot ( data . index * ( 1 / fs ), data . ECG , alpha = 0.5 , label = 'raw ECG data' ) plt . plot ( data . index * ( 1 / fs ), data . ECG_ma , label = 'moving average' ) plt . scatter ( peak . position * ( 1 / fs ), peak . value , marker = 'x' , color = 'red' , s = 70 , label = \"R-peak\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . legend ( framealpha = 0.6 ) plt . show () Output: . Step 4: Peak Validation \u2693\ufe0e From the above plot, it shows that the peak of QRS complex is detected, as well as the peak from T-wave and P-wave. However, we just want to keep the peak from the QRS complex. We can validate the peak to see if the peak is the one we would like to keep or to reject. The simplest way is to compare each peak value with the average peak value and then keep only those peaks with value greater than the average peak value. # compute the average peak values average_peak_value = peak . value . mean () # keep only the peaks that are greater than the average peak value val_peak = peak . loc [ peak . value > average_peak_value , :] plt . figure ( figsize = ( 12 , 3 )) plt . plot ( data . index * ( 1 / fs ), data . ECG , alpha = 0.5 , label = 'raw ECG data' ) plt . plot ( data . index * ( 1 / fs ), data . ECG_ma , label = 'moving average' ) plt . scatter ( val_peak . position * ( 1 / fs ), val_peak . value , marker = 'x' , color = 'red' , s = 70 , label = \"Validated R-peak\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . legend ( framealpha = 0.6 ) plt . show () Output: . Step 5: Heart Rate Computation \u2693\ufe0e First, compute the duration between each RR interval, and then use the duration to compute for the heart rate, i.e., \\[ HR = 60 \\frac{1}{d_{RR}}, \\qquad (bpm) \\] RR_duration = ( val_peak . position [ 1 :] . values - val_peak . position [: - 1 ] . values ) * 1 / fs HR = 60 / np . mean ( RR_duration ) print ( f 'Heart rate: { HR } bpm' ) Output: Heart rate: 70.09345794392523 bpm Wrap Up the R-peak Detection \u2693\ufe0e We can group those five steps into a single function that accepts the ECG data and sampling rate as input argument and return a list of tuple containing R-peak values and their corresponding positions. The benefit of using function is that we can reuse the function for different ECG data without rewriting the same code. Let's realize this function implementation for R-peak detection. def moving_average ( ecg , window ): return np . convolve ( ecg , np . ones ( window ), 'same' ) / window def rpeak_detection ( ecg , fs ): data = pd . DataFrame ( ecg , columns = [ 'ECG' ]) # 1. moving average data [ 'ECG_ma' ] = moving_average ( data . ECG , int ( fs / 2 )) # 2. find all the RoIs data [ 'roi' ] = np . zeros ( len ( data )) track_start = True for index , row in data . iterrows (): # check if the point is above the moving average point if ( row . ECG > row . ECG_ma ): if ( track_start ): k = int ( np . max ( data . roi ) + 1 ) track_start = False data . roi . iloc [ index ] = k else : track_start = True # 3. locate the peak for all RoIs groups = data . groupby ([ 'roi' ]) columns = [ 'roi' , 'value' , 'position' ] peak = pd . DataFrame ( columns = columns ) for k ,( roi , group ) in enumerate ( groups ): if ( roi != 0 ): peak = peak . append ( pd . Series ({ 'roi' : roi , 'value' : np . max ( group . ECG ), 'position' : group . index [ np . argmax ( group . ECG )] }, name = k - 1 )) # 4. validate the peak average_peak_value = peak . value . mean () val_peak = peak . loc [ peak . value > average_peak_value , :] return ( val_peak . position , val_peak . value ) Create a function to compute the heart rate given the list of R-peak positions as input argument. def cal_HR ( rpeak_position ): RR_duration = ( rpeak_position [ 1 :] . values - rpeak_position [: - 1 ] . values ) * 1 / fs return 60 / np . mean ( RR_duration ) Create a plotting function to visualize the detected R-peaks with respect to the ECG data. def plot_rpeak ( fs , ecg , peak_position , peak_value ): plt . plot ( np . arange ( 0 , len ( ecg )) * ( 1 / fs ), ecg , alpha = 0.5 , label = 'raw ECG data' ) plt . scatter ( peak_position * ( 1 / fs ), peak_value , marker = 'x' , color = 'red' , s = 70 , label = \"Validated R-peak\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . legend ( framealpha = 0.6 ) plt . show () We can now call the function by supplying the ECG data and the sampling rate. Besides trying with the ECG data we have previously, we can also try with another two ECG samples: sample-ecg-data-75 , sample-ecg-data-85 . Both samples are of 5 second duration with sampling rate equals to 100Hz. Note that the rpeak_detection function takes in a numpy array as input argument, so we got to be careful when supplying the ecg input to the function. # try with sample-ecg-data-70 fs = 100 data = pd . read_csv ( 'sample-ecg-data-70.csv' ) rpeaks = rpeak_detection ( data . ECG , fs ) plt . figure ( figsize = ( 12 , 3 )) plot_rpeak ( 100 , data . ECG , rpeaks [ 0 ], rpeaks [ 1 ]) # compute the heart rate HR = cal_HR ( rpeaks [ 0 ]) print ( f 'Heart rate: { HR } bpm' ) Output: . Output: Heart rate: 70.09345794392523 bpm # try with sample-ecg-data-75 fs = 100 data = pd . read_csv ( 'sample-ecg-data-75.csv' ) rpeaks = rpeak_detection ( data . ECG , fs ) plt . figure ( figsize = ( 12 , 3 )) plot_rpeak ( 100 , data . ECG , rpeaks [ 0 ], rpeaks [ 1 ]) # compute the heart rate HR = cal_HR ( rpeaks [ 0 ]) print ( f 'Heart rate: { HR } bpm' ) Output: . Output: Heart rate: 75.0 bpm # try with sample-ecg-data-85 fs = 100 data = pd . read_csv ( 'sample-ecg-data-85.csv' ) rpeaks = rpeak_detection ( data . ECG , fs ) plt . figure ( figsize = ( 12 , 3 )) plot_rpeak ( 100 , data . ECG , rpeaks [ 0 ], rpeaks [ 1 ]) # compute the heart rate HR = cal_HR ( rpeaks [ 0 ]) print ( f 'Heart rate: { HR } bpm' ) Output: . Output: Heart rate: 85.02024291497976 bpm Resources \u2693\ufe0e The complete code to the above implementation is available here . Many algorithms have been developed to compute the R-peak detection. Here you can find a collection of 7 ECG heartbeat detection algorithms implemented in Python. You can try out those peak detection algorithms by installing the package using pip install py-ecg-detectors .","title":"Heart Rate Measurement from ECG"},{"location":"biometric_signals/aboutecg/aboutecg/#electrocardiogram-ecg","text":"Electrocardiogram (ECG) measures the electrical activity in our heart. ECG machine is commonly used in hospital to monitor patient's heart rate. Nowadays, a few consumer-grade device has integrated the ECG sensor into their smartwatch, for example Apple Watch and Fitbit .","title":"Electrocardiogram (ECG)"},{"location":"biometric_signals/aboutecg/aboutecg/#ecg-morphology","text":"ECG morphology can be described by 5 waves: P-wave, Q-wave, R-wave, S-wave, and T-wave. P-wave reflects atrial depolarization. It is a small wave due to the small muscle mass made by atria. T-wave reflects the rapid repolarization of contractile cell. Normal T-wave is slightly asymmetric, with a steeper downward slope. Q-wave, R-wave, S-wave are knowns as QRS complex in general. The QRS complex represents the depolarization of the ventricles. A QRS complex with a short duration implies a rapid depolarization, proving that the conduction system functions properly. A QRS complex with a long duration implies a slow depolarization, showing a high likelihood the dysfunction in conduction system.","title":"ECG Morphology"},{"location":"biometric_signals/aboutecg/aboutecg/#determine-the-heart-rate-from-ecg","text":"The heart rate can be determined by measuring the duration of RR interval between two successive QRS complexes. Once the RR interval is known, the heart rate can be computed as follows: \\[ HR = 60 \\frac{1}{d_{RR}} \\] where HR is heart rate is bpm, and \\(d_{RR}\\) is the duration of RR interval. Hence, the first problem to heart rate estimation is to locate the position of R-peak.","title":"Determine the heart rate from ECG"},{"location":"biometric_signals/aboutecg/aboutecg/#r-peak-detection","text":"We will use a sample ECG data to demonstrate the R-peak detection, estimating the RR interval, and calculating the heart rate. This sample ECG data is generated artificially using neurokit2 . A 5 second data is generated with 100Hz sampling rate. The data is saved in csv file so we can use pandas to read the data. We can visualize the sample data using matplotlib. import pandas as pd import matplotlib.pyplot as plt import numpy as np fs = 100 data = pd . read_csv ( 'sample-ecg-data-70.csv' ) plt . figure ( figsize = ( 12 , 3 )) plt . plot ( data . index * ( 1 / fs ), data . ECG ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . show () Output: . From the plot, we can clearly identify the P-wave, T-wave, and the QRS complex. The goal is to locate all the R-peaks given the 5 second ECG signal. There are a number of ways to R-peaks detection, one of them is described in Halmiton algorithm Here, we will use a simple moving average approach to R-peak detection. The key steps of this approach is summarized as follows: compute the moving average with a window that is at least half the sampling frequency mark the region of interest (RoI) for those signals above the moving average locate the peak value and peak location for all RoIs calculate the average peak value of all peaks, and reject those peaks with peak value less than the average compute the duration between each RR-interval, and use the mean duration to compute the heart rate.","title":"R-peak Detection"},{"location":"biometric_signals/aboutecg/aboutecg/#step-1-moving-average","text":"The sampling rate is 100Hz, so half the sampling rate is 50Hz, which means 50 data points per one second. # 1. compute the moving average with window of 50 data points window = 50 # use the convolve function by numpy to compute the moving average data [ 'ECG_ma' ] = np . convolve ( data . ECG , np . ones ( window ), 'same' ) / window plt . figure ( figsize = ( 12 , 3 )) plt . plot ( data . index * ( 1 / fs ), data . ECG , alpha = 0.5 , label = 'raw ECG data' ) plt . plot ( data . index * ( 1 / fs ), data . ECG_ma , label = 'moving average' ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . legend ( framealpha = 0.6 ) plt . show () Output: .","title":"Step 1: Moving average"},{"location":"biometric_signals/aboutecg/aboutecg/#step-2-rois-identification","text":"From the above plot, we can see that some data points are above the moving average line, and some are below. We can mark those above the moving average line as RoI. # 2. find all the RoIs data [ 'roi' ] = np . zeros ( len ( data )) track_start = True for index , row in data . iterrows (): # check if the point is above the moving average point if ( row . ECG > row . ECG_ma ): if ( track_start ): k = int ( np . max ( data . roi ) + 1 ) print ( f '[ { k } ] RoI detected...' ) track_start = False data . roi . iloc [ index ] = k else : track_start = True Output: [1] RoI detected... [2] RoI detected... [3] RoI detected... [4] RoI detected... [5] RoI detected... [6] RoI detected... [7] RoI detected... [8] RoI detected... [9] RoI detected... [10] RoI detected... [11] RoI detected... [12] RoI detected... [13] RoI detected... [14] RoI detected... [15] RoI detected... [16] RoI detected... [17] RoI detected... [18] RoI detected...","title":"Step 2: RoIs Identification"},{"location":"biometric_signals/aboutecg/aboutecg/#step-3-peak-detection","text":"For each RoI, we can compute the peak value and peak position by taking the maximum value. # 3. locate the peak for all RoIs groups = data . groupby ([ 'roi' ]) columns = [ 'roi' , 'value' , 'position' ] peak = pd . DataFrame ( columns = columns ) for k ,( roi , group ) in enumerate ( groups ): if ( roi != 0 ): peak = peak . append ( pd . Series ({ 'roi' : roi , 'value' : np . max ( group . ECG ), 'position' : group . index [ np . argmax ( group . ECG )] }, name = k - 1 )) plt . figure ( figsize = ( 12 , 3 )) plt . plot ( data . index * ( 1 / fs ), data . ECG , alpha = 0.5 , label = 'raw ECG data' ) plt . plot ( data . index * ( 1 / fs ), data . ECG_ma , label = 'moving average' ) plt . scatter ( peak . position * ( 1 / fs ), peak . value , marker = 'x' , color = 'red' , s = 70 , label = \"R-peak\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . legend ( framealpha = 0.6 ) plt . show () Output: .","title":"Step 3: Peak Detection"},{"location":"biometric_signals/aboutecg/aboutecg/#step-4-peak-validation","text":"From the above plot, it shows that the peak of QRS complex is detected, as well as the peak from T-wave and P-wave. However, we just want to keep the peak from the QRS complex. We can validate the peak to see if the peak is the one we would like to keep or to reject. The simplest way is to compare each peak value with the average peak value and then keep only those peaks with value greater than the average peak value. # compute the average peak values average_peak_value = peak . value . mean () # keep only the peaks that are greater than the average peak value val_peak = peak . loc [ peak . value > average_peak_value , :] plt . figure ( figsize = ( 12 , 3 )) plt . plot ( data . index * ( 1 / fs ), data . ECG , alpha = 0.5 , label = 'raw ECG data' ) plt . plot ( data . index * ( 1 / fs ), data . ECG_ma , label = 'moving average' ) plt . scatter ( val_peak . position * ( 1 / fs ), val_peak . value , marker = 'x' , color = 'red' , s = 70 , label = \"Validated R-peak\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . legend ( framealpha = 0.6 ) plt . show () Output: .","title":"Step 4: Peak Validation"},{"location":"biometric_signals/aboutecg/aboutecg/#step-5-heart-rate-computation","text":"First, compute the duration between each RR interval, and then use the duration to compute for the heart rate, i.e., \\[ HR = 60 \\frac{1}{d_{RR}}, \\qquad (bpm) \\] RR_duration = ( val_peak . position [ 1 :] . values - val_peak . position [: - 1 ] . values ) * 1 / fs HR = 60 / np . mean ( RR_duration ) print ( f 'Heart rate: { HR } bpm' ) Output: Heart rate: 70.09345794392523 bpm","title":"Step 5: Heart Rate Computation"},{"location":"biometric_signals/aboutecg/aboutecg/#wrap-up-the-r-peak-detection","text":"We can group those five steps into a single function that accepts the ECG data and sampling rate as input argument and return a list of tuple containing R-peak values and their corresponding positions. The benefit of using function is that we can reuse the function for different ECG data without rewriting the same code. Let's realize this function implementation for R-peak detection. def moving_average ( ecg , window ): return np . convolve ( ecg , np . ones ( window ), 'same' ) / window def rpeak_detection ( ecg , fs ): data = pd . DataFrame ( ecg , columns = [ 'ECG' ]) # 1. moving average data [ 'ECG_ma' ] = moving_average ( data . ECG , int ( fs / 2 )) # 2. find all the RoIs data [ 'roi' ] = np . zeros ( len ( data )) track_start = True for index , row in data . iterrows (): # check if the point is above the moving average point if ( row . ECG > row . ECG_ma ): if ( track_start ): k = int ( np . max ( data . roi ) + 1 ) track_start = False data . roi . iloc [ index ] = k else : track_start = True # 3. locate the peak for all RoIs groups = data . groupby ([ 'roi' ]) columns = [ 'roi' , 'value' , 'position' ] peak = pd . DataFrame ( columns = columns ) for k ,( roi , group ) in enumerate ( groups ): if ( roi != 0 ): peak = peak . append ( pd . Series ({ 'roi' : roi , 'value' : np . max ( group . ECG ), 'position' : group . index [ np . argmax ( group . ECG )] }, name = k - 1 )) # 4. validate the peak average_peak_value = peak . value . mean () val_peak = peak . loc [ peak . value > average_peak_value , :] return ( val_peak . position , val_peak . value ) Create a function to compute the heart rate given the list of R-peak positions as input argument. def cal_HR ( rpeak_position ): RR_duration = ( rpeak_position [ 1 :] . values - rpeak_position [: - 1 ] . values ) * 1 / fs return 60 / np . mean ( RR_duration ) Create a plotting function to visualize the detected R-peaks with respect to the ECG data. def plot_rpeak ( fs , ecg , peak_position , peak_value ): plt . plot ( np . arange ( 0 , len ( ecg )) * ( 1 / fs ), ecg , alpha = 0.5 , label = 'raw ECG data' ) plt . scatter ( peak_position * ( 1 / fs ), peak_value , marker = 'x' , color = 'red' , s = 70 , label = \"Validated R-peak\" ) plt . xlabel ( 'Time (s)' ) plt . ylabel ( 'ECG Amplitude' ) plt . legend ( framealpha = 0.6 ) plt . show () We can now call the function by supplying the ECG data and the sampling rate. Besides trying with the ECG data we have previously, we can also try with another two ECG samples: sample-ecg-data-75 , sample-ecg-data-85 . Both samples are of 5 second duration with sampling rate equals to 100Hz. Note that the rpeak_detection function takes in a numpy array as input argument, so we got to be careful when supplying the ecg input to the function. # try with sample-ecg-data-70 fs = 100 data = pd . read_csv ( 'sample-ecg-data-70.csv' ) rpeaks = rpeak_detection ( data . ECG , fs ) plt . figure ( figsize = ( 12 , 3 )) plot_rpeak ( 100 , data . ECG , rpeaks [ 0 ], rpeaks [ 1 ]) # compute the heart rate HR = cal_HR ( rpeaks [ 0 ]) print ( f 'Heart rate: { HR } bpm' ) Output: . Output: Heart rate: 70.09345794392523 bpm # try with sample-ecg-data-75 fs = 100 data = pd . read_csv ( 'sample-ecg-data-75.csv' ) rpeaks = rpeak_detection ( data . ECG , fs ) plt . figure ( figsize = ( 12 , 3 )) plot_rpeak ( 100 , data . ECG , rpeaks [ 0 ], rpeaks [ 1 ]) # compute the heart rate HR = cal_HR ( rpeaks [ 0 ]) print ( f 'Heart rate: { HR } bpm' ) Output: . Output: Heart rate: 75.0 bpm # try with sample-ecg-data-85 fs = 100 data = pd . read_csv ( 'sample-ecg-data-85.csv' ) rpeaks = rpeak_detection ( data . ECG , fs ) plt . figure ( figsize = ( 12 , 3 )) plot_rpeak ( 100 , data . ECG , rpeaks [ 0 ], rpeaks [ 1 ]) # compute the heart rate HR = cal_HR ( rpeaks [ 0 ]) print ( f 'Heart rate: { HR } bpm' ) Output: . Output: Heart rate: 85.02024291497976 bpm","title":"Wrap Up the R-peak Detection"},{"location":"biometric_signals/aboutecg/aboutecg/#resources","text":"The complete code to the above implementation is available here . Many algorithms have been developed to compute the R-peak detection. Here you can find a collection of 7 ECG heartbeat detection algorithms implemented in Python. You can try out those peak detection algorithms by installing the package using pip install py-ecg-detectors .","title":"Resources"},{"location":"biometric_signals/aboutheartrate/aboutheartrate/","text":"Our Heart is beating \u2693\ufe0e We are living because our heart is beating. Our heat beats up and down to accommodate our body's need for oxygen. Heart rate is measured in beat per minute (bpm). A healthy person should have resting heart rates at about 60 to 100 bpm. The lower the heart rate, the more efficient heart function, and the better the cardiovascular fitness. A well-trained athlete, for example, can have a resting heart rate closer to 40 bpm. Maximum Heart Rate and Aerobic Capacity \u2693\ufe0e Maximum heart rate refers to the rate at which you heart is beating its hardest to meet your body's need for oxygen. Aerobic capacity refers to the amount of oxygen our body can consume. Our maximum heart rate is connected to our aerobic capacity - a high aerobic capacity results in a lower risk of heart attack. Figure below shows the maximum heart rate according to human's age. We can increase our maximum heart rate and aerobic capacity through vigorous excersie. While maintaining a maximum heart rate for more than a few minutes is almost impossible, physiologicts have set the percentage of maximum heart rate as target during exercise. It is advisable to set our target heart rate at 50% of our maximum heart rate when we are in the beginning of an exercise. After that, we can slowly increase the exercise's intensity until our heart rate reach 70% to 80%. How to measure your heart rate? \u2693\ufe0e Our wrist is probably the most convenient place to measure our heart rate. We can feel our pulse by placing our index and middle fingers between the bone and the tendor over our radial artery (i.e., on the fat pad thumb side of your wrist), as shown in the Figure below. We can obtain a pretty accurate heart rate measure by counting the number of beats in 15 seconds, and then multiply the number of beats by four. To estimate our maximum heart rate, we should measure our pulse immediately after exercising. Besides measuring your heart rate with your finger manually, nowadays we can monitor our heart rate constantly using a smartwatch. There are two types of sensor available on the smartwatch for measuring the heart rate, one is the optical heart rate sensor based on photoplethysmography sensing, and another is the electrical heart rate sensor based on electrocardiography sensing. Optical Heart Rate Sensor \u2693\ufe0e Photoplethysmogram (PPG) is an optical sensing technology. It measures the volumetric changes in blood circulation based on the amount of light being transmitted and reflected by the blood vessel. Almost every smartwatch is equipped with a PPG sensor using green LED lights. The green LED lights flash periodically - the green light is transmitted through the skin, and the amount of reflected light tells how fast your heart is beating. Electical Heart Rate Sensor \u2693\ufe0e Electrocardiogram (ECG) uses electrodes to measure the electrical activity of our heart. Specifically, it measures the electrical signals that make our heart beat, thus providing informat aobut our heart rate. There are only a few smartwatches on the market that provide heart rate measurement using ECG sensing.","title":"Heart Rate"},{"location":"biometric_signals/aboutheartrate/aboutheartrate/#our-heart-is-beating","text":"We are living because our heart is beating. Our heat beats up and down to accommodate our body's need for oxygen. Heart rate is measured in beat per minute (bpm). A healthy person should have resting heart rates at about 60 to 100 bpm. The lower the heart rate, the more efficient heart function, and the better the cardiovascular fitness. A well-trained athlete, for example, can have a resting heart rate closer to 40 bpm.","title":"Our Heart is beating"},{"location":"biometric_signals/aboutheartrate/aboutheartrate/#maximum-heart-rate-and-aerobic-capacity","text":"Maximum heart rate refers to the rate at which you heart is beating its hardest to meet your body's need for oxygen. Aerobic capacity refers to the amount of oxygen our body can consume. Our maximum heart rate is connected to our aerobic capacity - a high aerobic capacity results in a lower risk of heart attack. Figure below shows the maximum heart rate according to human's age. We can increase our maximum heart rate and aerobic capacity through vigorous excersie. While maintaining a maximum heart rate for more than a few minutes is almost impossible, physiologicts have set the percentage of maximum heart rate as target during exercise. It is advisable to set our target heart rate at 50% of our maximum heart rate when we are in the beginning of an exercise. After that, we can slowly increase the exercise's intensity until our heart rate reach 70% to 80%.","title":"Maximum Heart Rate and Aerobic Capacity"},{"location":"biometric_signals/aboutheartrate/aboutheartrate/#how-to-measure-your-heart-rate","text":"Our wrist is probably the most convenient place to measure our heart rate. We can feel our pulse by placing our index and middle fingers between the bone and the tendor over our radial artery (i.e., on the fat pad thumb side of your wrist), as shown in the Figure below. We can obtain a pretty accurate heart rate measure by counting the number of beats in 15 seconds, and then multiply the number of beats by four. To estimate our maximum heart rate, we should measure our pulse immediately after exercising. Besides measuring your heart rate with your finger manually, nowadays we can monitor our heart rate constantly using a smartwatch. There are two types of sensor available on the smartwatch for measuring the heart rate, one is the optical heart rate sensor based on photoplethysmography sensing, and another is the electrical heart rate sensor based on electrocardiography sensing.","title":"How to measure your heart rate?"},{"location":"biometric_signals/aboutheartrate/aboutheartrate/#optical-heart-rate-sensor","text":"Photoplethysmogram (PPG) is an optical sensing technology. It measures the volumetric changes in blood circulation based on the amount of light being transmitted and reflected by the blood vessel. Almost every smartwatch is equipped with a PPG sensor using green LED lights. The green LED lights flash periodically - the green light is transmitted through the skin, and the amount of reflected light tells how fast your heart is beating.","title":"Optical Heart Rate Sensor"},{"location":"biometric_signals/aboutheartrate/aboutheartrate/#electical-heart-rate-sensor","text":"Electrocardiogram (ECG) uses electrodes to measure the electrical activity of our heart. Specifically, it measures the electrical signals that make our heart beat, thus providing informat aobut our heart rate. There are only a few smartwatches on the market that provide heart rate measurement using ECG sensing.","title":"Electical Heart Rate Sensor"},{"location":"blog/research/","text":"..","title":"Research"},{"location":"blog/tools/diplaying-image-with-opencv-inside-jupyterlab/Displaying-Image-with-OpenCV-inside-JupyterLab/","text":"Displaying an Image with OpenCV inside Jupyter Lab \u2693\ufe0e cv2.imshow is often used to display image. However, calling cv2.imshow in Jupyter Lab will fire up a new window. Displaying an Image with cv2.imshow \u2693\ufe0e I took some photos when I visited Melaka (a small state in Malaysia) by myself in year 2020 (When coronavirus hasn't hit Malaysia yet). I can use the cv2.imread and cv2.imshow to read the image. import cv2 melaka_img = cv2 . imread ( 'melaka.jpg' ) cv2 . imshow ( 'Window' , melaka_img ) cv2 . waitKey ( 0 ) cv2 . destroyAllWindows () . A large window popped up and show the image of Melaka. However, the size of the image is too big to fit in my single computer display, and there is no way to adjust the size of the window. Even though I connected my computer to another two external displays, some parts of the image are still not visible. Displaying an Image within the Jupyter Lab \u2693\ufe0e There are a few methods to display an image read by openCV inside Jupyter Lab. The common way is to use the imshow in Matplotlib. In this post, we are going to look at another way to display the image inside the Jupyter Lab. Since Jupyter Lab is working within a web browser, we can use the HTML method to display the image inside the Jupyter Lab. Let's import the related dependencies. from IPython.display import display , HTML import base64 import cv2 Next, let's define a function that takes in the image name, the image data nad the display size as the input arguments and return an HTML content. def imshow ( name , img , size ): _ , img = cv2 . imencode ( '.jpg' , img ) encoded = base64 . b64encode ( img ) return HTML ( data = '''<img alt=\" {0} \" src=\"data:image/img;base64, {1} \" style=\"width: {2} px\"/>''' . format ( name , encoded . decode ( 'ascii' ), size )) Now we can call the imshow function that we have defined to display the image inside Jupyter Lab. melaka_img = cv2 . imread ( 'melaka.jpg' ) imshow ( 'Melaka' , melaka_img , 200 ) Let's change the size and see the output. imshow ( 'Melaka' , melaka_img , 500 ) Note \u2693\ufe0e The imshow function can be further customized by including some style properties to define the image's style.","title":"Image inside Jupyter Lab"},{"location":"blog/tools/diplaying-image-with-opencv-inside-jupyterlab/Displaying-Image-with-OpenCV-inside-JupyterLab/#displaying-an-image-with-opencv-inside-jupyter-lab","text":"cv2.imshow is often used to display image. However, calling cv2.imshow in Jupyter Lab will fire up a new window.","title":"Displaying an Image with OpenCV inside Jupyter Lab"},{"location":"blog/tools/diplaying-image-with-opencv-inside-jupyterlab/Displaying-Image-with-OpenCV-inside-JupyterLab/#displaying-an-image-with-cv2imshow","text":"I took some photos when I visited Melaka (a small state in Malaysia) by myself in year 2020 (When coronavirus hasn't hit Malaysia yet). I can use the cv2.imread and cv2.imshow to read the image. import cv2 melaka_img = cv2 . imread ( 'melaka.jpg' ) cv2 . imshow ( 'Window' , melaka_img ) cv2 . waitKey ( 0 ) cv2 . destroyAllWindows () . A large window popped up and show the image of Melaka. However, the size of the image is too big to fit in my single computer display, and there is no way to adjust the size of the window. Even though I connected my computer to another two external displays, some parts of the image are still not visible.","title":"Displaying an Image with cv2.imshow"},{"location":"blog/tools/diplaying-image-with-opencv-inside-jupyterlab/Displaying-Image-with-OpenCV-inside-JupyterLab/#displaying-an-image-within-the-jupyter-lab","text":"There are a few methods to display an image read by openCV inside Jupyter Lab. The common way is to use the imshow in Matplotlib. In this post, we are going to look at another way to display the image inside the Jupyter Lab. Since Jupyter Lab is working within a web browser, we can use the HTML method to display the image inside the Jupyter Lab. Let's import the related dependencies. from IPython.display import display , HTML import base64 import cv2 Next, let's define a function that takes in the image name, the image data nad the display size as the input arguments and return an HTML content. def imshow ( name , img , size ): _ , img = cv2 . imencode ( '.jpg' , img ) encoded = base64 . b64encode ( img ) return HTML ( data = '''<img alt=\" {0} \" src=\"data:image/img;base64, {1} \" style=\"width: {2} px\"/>''' . format ( name , encoded . decode ( 'ascii' ), size )) Now we can call the imshow function that we have defined to display the image inside Jupyter Lab. melaka_img = cv2 . imread ( 'melaka.jpg' ) imshow ( 'Melaka' , melaka_img , 200 ) Let's change the size and see the output. imshow ( 'Melaka' , melaka_img , 500 )","title":"Displaying an Image within the Jupyter Lab"},{"location":"blog/tools/diplaying-image-with-opencv-inside-jupyterlab/Displaying-Image-with-OpenCV-inside-JupyterLab/#note","text":"The imshow function can be further customized by including some style properties to define the image's style.","title":"Note"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/","text":"Jupyter Lab \u2693\ufe0e JupyterLab is the next-generation Jupyter notebook with more comprehensive user interface. The interface is highly modular and customizable. If you have worked with JupyterLab, you might be realized that the JupyterLab only consists of the virtual environment you created through Anaconda, but those virtual environments created through venv or virtualenv are not found inside the launcher. While IPython kernel is always available in JupyterLab, other version of Python and virtual environment might not available when you start the JupyterLab. That's being said, we got to add the virtual environment to JupyterLab manually. Setting up Virtual Environment \u2693\ufe0e To add the virtual environment, first, make sure that the virtual environment is created and activated. For windows, we can create the virtual environment with PowerShell by typing the following command: python -m venv env Next, we need to activate the virtual environment (i.e., the env that we have created). env/Scripts/activate Check if ipykernel is installed, we can use pip list to list down a list of installed packages. . If ipykernel is not installed, install the ipykernel which provides the IPython kernel for JupyterLab. pip install ipykernel Adding Virtual Environment \u2693\ufe0e Once it is installed, we can add the virtual environment we created just now to JupyterLab. To add the virtual environment, enter the following command: python -m ipykernel install --name=env You should get the following output: . The output said that the kernelspec env is installed in the above directory (i.e., C:\\ProgramData\\jupyter\\kernels\\env in this case). Navigate to the said directory, you can see that the folder containing your environment spec. . Inside the folder, you will find a kernel.json file containing the information of your virtual environment env . . If you open the kernel.json , you should see that it contains the following information: . Now, if you open the JupyterLab again, you can find your virtual environment env inside the Launcher . Remove the manually added Kernelspec from JupyterLab \u2693\ufe0e You can check the list of kernels you have in your JupyterLab with the following command: jupyter kernelspec list You should see env inside the list. . If you would like to remove the virtual environment env from JupyterLab, type in the following command to uninstall the kernelspec. jupyter kernelspec uninstall env . The output shows that the kernelspec is successfully removed.","title":"Adding Virtual Environment to Jupyter Lab"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/#jupyter-lab","text":"JupyterLab is the next-generation Jupyter notebook with more comprehensive user interface. The interface is highly modular and customizable. If you have worked with JupyterLab, you might be realized that the JupyterLab only consists of the virtual environment you created through Anaconda, but those virtual environments created through venv or virtualenv are not found inside the launcher. While IPython kernel is always available in JupyterLab, other version of Python and virtual environment might not available when you start the JupyterLab. That's being said, we got to add the virtual environment to JupyterLab manually.","title":"Jupyter Lab"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/#setting-up-virtual-environment","text":"To add the virtual environment, first, make sure that the virtual environment is created and activated. For windows, we can create the virtual environment with PowerShell by typing the following command: python -m venv env Next, we need to activate the virtual environment (i.e., the env that we have created). env/Scripts/activate Check if ipykernel is installed, we can use pip list to list down a list of installed packages. . If ipykernel is not installed, install the ipykernel which provides the IPython kernel for JupyterLab. pip install ipykernel","title":"Setting up Virtual Environment"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/#adding-virtual-environment","text":"Once it is installed, we can add the virtual environment we created just now to JupyterLab. To add the virtual environment, enter the following command: python -m ipykernel install --name=env You should get the following output: . The output said that the kernelspec env is installed in the above directory (i.e., C:\\ProgramData\\jupyter\\kernels\\env in this case). Navigate to the said directory, you can see that the folder containing your environment spec. . Inside the folder, you will find a kernel.json file containing the information of your virtual environment env . . If you open the kernel.json , you should see that it contains the following information: . Now, if you open the JupyterLab again, you can find your virtual environment env inside the Launcher .","title":"Adding Virtual Environment"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/#remove-the-manually-added-kernelspec-from-jupyterlab","text":"You can check the list of kernels you have in your JupyterLab with the following command: jupyter kernelspec list You should see env inside the list. . If you would like to remove the virtual environment env from JupyterLab, type in the following command to uninstall the kernelspec. jupyter kernelspec uninstall env . The output shows that the kernelspec is successfully removed.","title":"Remove the manually added Kernelspec from JupyterLab"},{"location":"bluetooth/bluetooth/","text":"--","title":"About Bluetooth"},{"location":"bluetooth/contact_tracing/2021-04-15-contactTracing/","text":"Last year about the same time, we developed a project, smart contact tracing based on BLE signals, in view of the viral spread of Covid-19. The project is funded by NSERC Alliance COVID-19 . jpg Here are a collection of news related to our project: https://www.guelphmercury.com/community-story/10038008-u-of-guelph-says-they-can-improve-accuracy-and-privacy-of-ontario-s-covid-19-contact-tracing-app/ https://www.guelphmercury.com/community-story/10038008-u-of-guelph-says-they-can-improve-accuracy-and-privacy-of-ontario-s-covid-19-contact-tracing-app/ https://mobilesyrup.com/2020/06/20/university-guelph-contact-tracing-app-machine-learning-accuracy/ https://porticomagazine.ca/2020/10/u-of-g-app-could-improve-covid-19-contact-tracing/ https://www.kitchenertoday.com/regional-news/engineers-at-university-of-guelph-develop-covid-19-tracing-app-2511739 https://guides.uoguelph.ca/2020/06/u-of-g-contact-tracing-app-could-help-improve-accuracy-of-the-technology/ https://issuu.com/uofguelph/docs/portico_magazine_fall_2020 This work is published in the following IEEE Journal:","title":"Smart Contact Tracing Project at University of Guelph"},{"location":"programming/matlab/","text":"--","title":"Matlab"},{"location":"programming/python/2021-04-18-flattenRavel/","text":"Dealing with multidimensional array with Numpy is quite intuitive. While doing an element-wise multipilication computation with Numpy is basically more easier compared to using a classical programming with a for loop, understanding the internal operation of Numpy is useful to improve the computation efficiency when dealing with arrays consists of millions of elements. In this post, we will look at the differences between flatten and ravel functions. Both functions provide the same one-dimensional output by stacking up a multidimensional inputs. The key difference is how the memory is copied during the process. Let's say we would like to flatten an 1000*1000 dimensional array, using flatten will returns a copy, whereas using ravel will returns a view. The computation time of both functions is shown as follows: import numpy as np # create a 1000*1000 dimensional array arr = np . random . rand ( 1000 , 1000 ) print ( f 'Size of arr: { arr . shape } ' ) Size of arr: (1000, 1000) %% time arr_flatten = arr . flatten () Wall time: 3.95 ms %% time arr_ravel = arr . ravel () Wall time: 0 ns Obviously, ravel is much more faster than flatten . Such a performance speedup can be significant when leading with very large arrays. We can also check that both ravel and flatten functions returns the same output. print ( np . array_equal ( arr_flatten , arr_ravel )) True The difference is that there is not copy operation with ravel . For flatten , the output is a copy of the original array; wheareas for ravel , the output is just a view of original array, in which whatever the changes in the second array will affect the change in the original array. To understand the difference between a copy and a view, consider the memory block of these three arrays. print ( f 'Memory address to store arr: { arr . __array_interface__ [ \"data\" ][ 0 ] } ' ) print ( f 'Memory address to store arr_flatten: { arr_flatten . __array_interface__ [ \"data\" ][ 0 ] } ' ) print ( f 'Memory address to store arr_ravel: { arr_ravel . __array_interface__ [ \"data\" ][ 0 ] } ' ) Memory address to store arr: 2394595090496 Memory address to store arr_flatten: 2394603155520 Memory address to store arr_ravel: 2394595090496 We can see that flatten uses copy the array to a new memory block; whreas ravel simply creates a view to the original array. When array is not in C-order \u2693\ufe0e Note that ravel will also do a copy operation when dealing with array that is not in the C-order. For example, when we consider the array in Fortrain-order, as in a.T , ravel actually returns a flattened version with C-order. %% time arr_ravel2 = arr . ravel () Wall time: 0 ns %% time arr_ravel2T = arr . T . ravel () Wall time: 7.07 ms When dealing with the array in different order, we can specify the order with ravel isntead. %% time arr_ravel3T = arr . ravel ( order = 'F' ) Wall time: 6.03 ms By specifying the order directly within the ravel function, the computation time is slightly faster than doing ravel directly on arr.T .","title":"Flatten vs Ravel functions in Numpy"},{"location":"programming/python/2021-04-18-flattenRavel/#when-array-is-not-in-c-order","text":"Note that ravel will also do a copy operation when dealing with array that is not in the C-order. For example, when we consider the array in Fortrain-order, as in a.T , ravel actually returns a flattened version with C-order. %% time arr_ravel2 = arr . ravel () Wall time: 0 ns %% time arr_ravel2T = arr . T . ravel () Wall time: 7.07 ms When dealing with the array in different order, we can specify the order with ravel isntead. %% time arr_ravel3T = arr . ravel ( order = 'F' ) Wall time: 6.03 ms By specifying the order directly within the ravel function, the computation time is slightly faster than doing ravel directly on arr.T .","title":"When array is not in C-order"},{"location":"programming/python/histogramVis/","text":"Histogram allows us to visualize the frequency distribution of our data. It breaks the data into a few smaller bins according to the value of the data, and then count the number of occurences (i.e., the frequency) in each bin. We can obtain the frequency and bins for a given data using the histogram() function from numpy. Let's consider the following example: import numpy as np # generate 1000 random numbers x = np . random . rand ( 1000 , 1 ) # count the occurences in each bin in x frequency , bins = np . histogram ( x , bins = 10 , range = [ 0 , 1 ]) for b , f in zip ( bins [ 1 :], frequency ): print ( f 'value: { ( round ( b , 1 )) } >> frequency: { f } ' ) Output: value: 0.1 >> frequency: 80 value: 0.2 >> frequency: 80 value: 0.3 >> frequency: 98 value: 0.4 >> frequency: 91 value: 0.5 >> frequency: 123 value: 0.6 >> frequency: 97 value: 0.7 >> frequency: 105 value: 0.8 >> frequency: 102 value: 0.9 >> frequency: 117 value: 1.0 >> frequency: 107 Here, we used numpy.random.rand() function to generate 1000 uniformly distributed values, ranging from 0 to 1. An array x is defined to store the generated values. We would like to know how many data is within 0-0.1, how many occurs at 0.1-0.2, and so on. These can be obtained by calling numpy.histogram() function. Histogram with Matplotlib \u2693\ufe0e Matplotlib allows us to plot the histogram with pyplot.hist() function. Let's continue with the above example, and use the histogram function in Matplotlib to visualize the data distribution. import matplotlib.pyplot as plt plt . hist ( x , bins = 10 ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . show () svg Let's generate another set of random number, but with normal distribution. Instead of numpy.random.rand() , we can use numpy.random.randn() to generate a series of values that follow standard normal distribution with zero mean and standard deviation equals to 1. Notes: if we would like to have a normal distribution with specific mean and standard deviation, we can use the following formula: $$ \\sigma * numpy.random.randn() + \\mu $$ # generate 1000 random numbers x = np . random . randn ( 1000 , 1 ) plt . hist ( x , bins = 10 ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . show () svg Multiple Histograms in a Single Plot \u2693\ufe0e We can plot multiple histograms for easy comparison. Let's create 3 numpy arrays each consists of 1000 normally distributed random numbers based on different mean and standard deviation. x1 = 3 * np . random . randn ( 1000 , 1 ) + 3 x2 = 2 * np . random . randn ( 1000 , 1 ) + 7 x3 = x plt . figure ( figsize = ( 12 , 5 )) plt . hist ( x1 , bins = 10 , alpha = 0.5 , color = 'red' , label = 'x1' ) plt . hist ( x2 , bins = 10 , alpha = 0.5 , color = 'green' , label = 'x2' ) plt . hist ( x3 , bins = 10 , alpha = 0.5 , color = 'blue' , label = 'x3' ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . legend () plt . show () svg Density Distribution \u2693\ufe0e Instead of using the number of occurences as the y-axis, we can normalize the occurences frequency by setting density to True , as shown below: plt . figure () plt . hist ( x , bins = 10 , density = True ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg Style the Histogram \u2693\ufe0e We can create spacing between each bin in the histogram using the set_style() function from seaborn. Note that seaborn is built upon matplotlib, so we can use seaborn and matplotlib together. import seaborn as sns sns . set_style ( \"white\" ) plt . figure () plt . hist ( x , bins = 10 , density = True ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg We can also use the seaborn.histplot() function to visualize the histogram and density curve on the same plot. plt . figure () # sns.histplot(x, bins = 10, hist_kws={'alpha': 0.5}, kde_kws={'linewidth': 2}) sns . histplot ( x , bins = 10 , alpha = 0.5 , stat = \"probability\" , kde = True , legend = False ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg","title":"Visualize Data Distribution with Histogram"},{"location":"programming/python/histogramVis/#histogram-with-matplotlib","text":"Matplotlib allows us to plot the histogram with pyplot.hist() function. Let's continue with the above example, and use the histogram function in Matplotlib to visualize the data distribution. import matplotlib.pyplot as plt plt . hist ( x , bins = 10 ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . show () svg Let's generate another set of random number, but with normal distribution. Instead of numpy.random.rand() , we can use numpy.random.randn() to generate a series of values that follow standard normal distribution with zero mean and standard deviation equals to 1. Notes: if we would like to have a normal distribution with specific mean and standard deviation, we can use the following formula: $$ \\sigma * numpy.random.randn() + \\mu $$ # generate 1000 random numbers x = np . random . randn ( 1000 , 1 ) plt . hist ( x , bins = 10 ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . show () svg","title":"Histogram with Matplotlib"},{"location":"programming/python/histogramVis/#multiple-histograms-in-a-single-plot","text":"We can plot multiple histograms for easy comparison. Let's create 3 numpy arrays each consists of 1000 normally distributed random numbers based on different mean and standard deviation. x1 = 3 * np . random . randn ( 1000 , 1 ) + 3 x2 = 2 * np . random . randn ( 1000 , 1 ) + 7 x3 = x plt . figure ( figsize = ( 12 , 5 )) plt . hist ( x1 , bins = 10 , alpha = 0.5 , color = 'red' , label = 'x1' ) plt . hist ( x2 , bins = 10 , alpha = 0.5 , color = 'green' , label = 'x2' ) plt . hist ( x3 , bins = 10 , alpha = 0.5 , color = 'blue' , label = 'x3' ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . legend () plt . show () svg","title":"Multiple Histograms in a Single Plot"},{"location":"programming/python/histogramVis/#density-distribution","text":"Instead of using the number of occurences as the y-axis, we can normalize the occurences frequency by setting density to True , as shown below: plt . figure () plt . hist ( x , bins = 10 , density = True ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg","title":"Density Distribution"},{"location":"programming/python/histogramVis/#style-the-histogram","text":"We can create spacing between each bin in the histogram using the set_style() function from seaborn. Note that seaborn is built upon matplotlib, so we can use seaborn and matplotlib together. import seaborn as sns sns . set_style ( \"white\" ) plt . figure () plt . hist ( x , bins = 10 , density = True ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg We can also use the seaborn.histplot() function to visualize the histogram and density curve on the same plot. plt . figure () # sns.histplot(x, bins = 10, hist_kws={'alpha': 0.5}, kde_kws={'linewidth': 2}) sns . histplot ( x , bins = 10 , alpha = 0.5 , stat = \"probability\" , kde = True , legend = False ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg","title":"Style the Histogram"},{"location":"tags/","text":"Tags \u2693\ufe0e","title":"Tags"},{"location":"tags/#tags","text":"","title":"Tags"}]}