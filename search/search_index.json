{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Pc's Blog \u2693\ufe0e It is alright to be imperfect. .welcome { padding-left: .1em; margin-bottom: 0 } .site-name { margin-bottom: .5em !important; color: blue !important; } .site-description { font-size: large; padding-left: .05em; margin-bottom: 0; } .md-typeset .md-button { font-size: unset; min-width: 3em; text-align: center; padding: .3em 0 0 0; border-radius: .5em; border: 1px solid lightgray; color: unset; }","title":"Home"},{"location":"#pcs-blog","text":"It is alright to be imperfect. .welcome { padding-left: .1em; margin-bottom: 0 } .site-name { margin-bottom: .5em !important; color: blue !important; } .site-description { font-size: large; padding-left: .05em; margin-bottom: 0; } .md-typeset .md-button { font-size: unset; min-width: 3em; text-align: center; padding: .3em 0 0 0; border-radius: .5em; border: 1px solid lightgray; color: unset; }","title":"Pc's Blog"},{"location":"blog/research/","text":"..","title":"Research"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/","text":"Jupyter Lab \u2693\ufe0e JupyterLab is the next-generation Jupyter notebook with more comprehensive user interface. The interface is highly modular and customizable. If you have worked with JupyterLab, you might be realized that the JupyterLab only consists of the virtual environment you created through Anaconda, but those virtual environments created through venv or virtualenv are not found inside the launcher. While IPython kernel is always available in JupyterLab, other version of Python and virtual environment might not available when you start the JupyterLab. That's being said, we got to add the virtual environment to JupyterLab manually. Setting up Virtual Environment \u2693\ufe0e To add the virtual environment, first, make sure that the virtual environment is created and activated. For windows, we can create the virtual environment with PowerShell by typing the following command: python -m venv env Next, we need to activate the virtual environment (i.e., the env that we have created). env/Scripts/activate Check if ipykernel is installed, we can use pip list to list down a list of installed packages. jpg If ipykernel is not installed, install the ipykernel which provides the IPython kernel for JupyterLab. pip install ipykernel Adding Virtual Environment \u2693\ufe0e Once it is installed, we can add the virtual environment we created just now to JupyterLab. To add the virtual environment, enter the following command: python -m ipykernel install --name=env You should get the following output: JPG The output said that the kernelspec env is installed in the above directory (i.e., C:\\ProgramData\\jupyter\\kernels\\env in this case). Navigate to the said directory, you can see that the folder containing your environment spec. JPG Inside the folder, you will find a kernel.json file containing the information of your virtual environment env . JPG If you open the kernel.json , you should see that it contains the following information: JPG Now, if you open the JupyterLab again, you can find your virtual environment env inside the Launcher JPG Remove the manually added Kernelspec from JupyterLab \u2693\ufe0e You can check the list of kernels you have in your JupyterLab with the following command: jupyter kernelspec list You should see env inside the list. JPG If you would like to remove the virtual environment env from JupyterLab, type in the following command to uninstall the kernelspec. jupyter kernelspec uninstall env JPG The output shows that the kernelspec is successfully removed.","title":"Adding Virtual Environment to Jupyter Lab"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/#jupyter-lab","text":"JupyterLab is the next-generation Jupyter notebook with more comprehensive user interface. The interface is highly modular and customizable. If you have worked with JupyterLab, you might be realized that the JupyterLab only consists of the virtual environment you created through Anaconda, but those virtual environments created through venv or virtualenv are not found inside the launcher. While IPython kernel is always available in JupyterLab, other version of Python and virtual environment might not available when you start the JupyterLab. That's being said, we got to add the virtual environment to JupyterLab manually.","title":"Jupyter Lab"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/#setting-up-virtual-environment","text":"To add the virtual environment, first, make sure that the virtual environment is created and activated. For windows, we can create the virtual environment with PowerShell by typing the following command: python -m venv env Next, we need to activate the virtual environment (i.e., the env that we have created). env/Scripts/activate Check if ipykernel is installed, we can use pip list to list down a list of installed packages. jpg If ipykernel is not installed, install the ipykernel which provides the IPython kernel for JupyterLab. pip install ipykernel","title":"Setting up Virtual Environment"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/#adding-virtual-environment","text":"Once it is installed, we can add the virtual environment we created just now to JupyterLab. To add the virtual environment, enter the following command: python -m ipykernel install --name=env You should get the following output: JPG The output said that the kernelspec env is installed in the above directory (i.e., C:\\ProgramData\\jupyter\\kernels\\env in this case). Navigate to the said directory, you can see that the folder containing your environment spec. JPG Inside the folder, you will find a kernel.json file containing the information of your virtual environment env . JPG If you open the kernel.json , you should see that it contains the following information: JPG Now, if you open the JupyterLab again, you can find your virtual environment env inside the Launcher JPG","title":"Adding Virtual Environment"},{"location":"blog/tools/jupyter_lab_virtual_env/2021-05-13-jupyterLabVirtualEnv/#remove-the-manually-added-kernelspec-from-jupyterlab","text":"You can check the list of kernels you have in your JupyterLab with the following command: jupyter kernelspec list You should see env inside the list. JPG If you would like to remove the virtual environment env from JupyterLab, type in the following command to uninstall the kernelspec. jupyter kernelspec uninstall env JPG The output shows that the kernelspec is successfully removed.","title":"Remove the manually added Kernelspec from JupyterLab"},{"location":"bluetooth/bluetooth/","text":"--","title":"About Bluetooth"},{"location":"bluetooth/contact_tracing/2021-04-15-contactTracing/","text":"Last year about the same time, we developed a project, smart contact tracing based on BLE signals, in view of the viral spread of Covid-19. The project is funded by NSERC Alliance COVID-19 . jpg Here are a collection of news related to our project: https://www.guelphmercury.com/community-story/10038008-u-of-guelph-says-they-can-improve-accuracy-and-privacy-of-ontario-s-covid-19-contact-tracing-app/ https://www.guelphmercury.com/community-story/10038008-u-of-guelph-says-they-can-improve-accuracy-and-privacy-of-ontario-s-covid-19-contact-tracing-app/ https://mobilesyrup.com/2020/06/20/university-guelph-contact-tracing-app-machine-learning-accuracy/ https://porticomagazine.ca/2020/10/u-of-g-app-could-improve-covid-19-contact-tracing/ https://www.kitchenertoday.com/regional-news/engineers-at-university-of-guelph-develop-covid-19-tracing-app-2511739 https://guides.uoguelph.ca/2020/06/u-of-g-contact-tracing-app-could-help-improve-accuracy-of-the-technology/ https://issuu.com/uofguelph/docs/portico_magazine_fall_2020 This work is published in the following IEEE Journal:","title":"Smart Contact Tracing Project at University of Guelph"},{"location":"deep_learning/","text":"-- ..","title":"Deep Learning"},{"location":"deep_learning/timeseries/dtw/","text":"Dynamic Time Warping (DTW) is a non-linear similarity computation method that dynamically compute the similarity between time series data when the time indices between data points from time series A and time series B do not match. Consider the two time series sequences (time series A in green, and time series B in blue) shown in Figure below, these two sequences do not line up in time axis. However, both of them have some similarities in terms of their component shapes. If traditional distance metrics (e.g., Euclidean distance, Manhattan distance, etc.) are used to compute the distance between the \\(i\\) th point of time series A with the \\(i\\) th point of time series B, it will most probably produce a poor similarity score. If a non-linear mapping can be used to match the similar shape of two time series even though these two time series sequences are out of phase in the time axis, it will produce a more intuitive similarity measure. This can be done by warping the time axis of one sequences to align the time axis. DTW can efficiently align two time series sequences, allowing a more intuitive similarity measure between out of sync data points. Warping Function \u2693\ufe0e DTW uses warping function to find the best alignment between two time series sequences. The objective is to find the path through the grids \\[ P = P_1, P_2, ..., P_s, ..., P_k \\] \\[ P_s = (i, j) \\] which minimizes the total distance between them. DTW aims to learn a warping path that dynamically maps the data points of time series A to data points of time series B. Let \\(A\\) and \\(B\\) be time series A and B, respectively, we can compute the time normalized distance between these two time series: \\[ D(A, B) = \\frac{\\sum_{s=1}^k d(P_s) w_s}{\\sum_{s=1}^k w_s} \\] where \\(d(P_s) = d(v_i, v_j)\\) is the distance between value at \\(i\\) th point \\(v_i\\) and value at \\(j\\) th point \\(v_j\\) , and \\(w_s\\) is the weight coefficient. Considering the Figure above, there are many possible warping paths through the grid. That's being said, to search for an optimum path, i.e., \\[ P_o = arg\\min_P (D(A, B)) \\] can be extremely hard when the grid size is big. Furthermore, if a single point of time series A can map onto a large subsection of time series B, it will lead to an unintuitive alignment. Over the decade, a few constraints have been imposed on the warping function: 1. Monotonicity \u2693\ufe0e Monotonicity ensures that the warping path does not go back in time. Given \\(P_s = (i, j)\\) and \\(P_{s-1} = (i', j')\\) , \\(i \\leq i'\\) and \\(j \\leq j'\\) forces the points in \\(P\\) to be monotonically spaced in time 2. Continuity \u2693\ufe0e Continuity ensures that the warping path does not jump in time. Given \\(P_s = (i, j)\\) and \\(P_{s-1} = (i', j')\\) , \\(i - i' \\leq 1\\) and \\(j - j' \\leq 1\\) restricts the allowable steps in the warping path to adjacent cells. 3. Boundary Conditions \u2693\ufe0e The boundary of the warping path states that the part should start at \\(P_1 = (1, 1)\\) and end at \\(P_s = (m, n)\\) , i.e., the warping path needs to start and finish in diagonally opposite corner of the grid. This is important to make sure the warping path does not consider only partial of the sequence. 4. Warping Window \u2693\ufe0e Let \\(r>0\\) be the length of warping window, \\(\\|i - j\\| \\leq r\\) restricts allowable grid points for the warping path. The warping window ensures that the warping path does not wander too far away from the diagonal. This guarantees that the alignment will not get stuck at similar features. 5. Slope Constraint \u2693\ufe0e Slope constraint ensures that the warping path is neither too steep or too shallow. Let \\(q\\) and \\(p\\) be the number of steps in the x-direction and y-direction given the grid, then \\(\\frac{j_{p} - j_{0}}{i_{p} - i_{0}} \\leq p\\) and \\(\\frac{i_{q} - i_{0}}{j_{q} - j_{0}} \\leq q\\) . That is, after \\(q\\) steps in x one must step in y and vice versa. Dynamic Programming \u2693\ufe0e Dynamic Programming is an efficient method to find the warping path. In general, dynamic programming evaluate the cumulative distance \\(\\gamma(i, j)\\) based on the distance \\(d(P_s)\\) at the current cell and the minimum of the cumulative distances of the adjacent elements. Mathematically, \\[ \\gamma(i, j) = d(P_s) + \\min \\{\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\} \\] where \\(d(P_s) = d(v_i, v_j)\\) can be calculated by taking the absolute difference between value at \\(i\\) th point and value at \\(j\\) th point, i.e., \\(d(v_i, v_j) = |v_i - v_j|\\) Example \u2693\ufe0e Suppose that we have two time series A and B as follows: \\[ A = [ 3, 2, 2, 3, 5, 5, 6 ] \\] \\[ B = [ 1, 3, 2, 2, 3, 5 ] \\] First, let's consider point \\(P_s = (1,1)\\) , i.e., \\(i=1\\) and \\(j=1\\) , the value at \\(i=1\\) is \\(v(i=1) = 3\\) and the value at \\(j=1\\) is \\(v(j=1) = 1\\) . Hence, the absolute distance at point \\(P_s = (1,1)\\) is \\(d(P_s) = |3-1| = 2\\) . Since the values at the cell above, left and diagonally above \\(P_s\\) are all empty, hence, \\(\\min \\{\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\} = 0\\) . The cumulative distance \\(\\gamma(i=1, j=1)\\) will be 2, as shown in Figure below. Let's consider another point \\(P_s = (4,4)\\) , here we have \\(v(i=4) = 3\\) and \\(v(j=4) = 2\\) . Hence, the absolute distance at point \\(P_s = (4,4)\\) is \\(d(P_s) = |3-2| = 1\\) . The values at the adjacent cells (above, left and diagonally above ) are 3, 2 and 2. By taking the minimum value, we have 2. So, the cumulative distance \\(\\gamma(i=4, j=4)\\) will be 3, as shown in Figure below. Figure below shows the final grid (or matrix) with all the value computed. The warping path can be obtained by backtracking from the end point to the beginning point as shown below. Implementation \u2693\ufe0e Here we will walkthrough a simple implementation of above example in Python. First, define the two time series A and B A = np . array ([ 3 , 2 , 2 , 3 , 5 , 5 , 6 ]) B = np . array ([ 1 , 3 , 2 , 2 , 3 , 5 ]) Initialize a dtw matrix based on the length of A and B, i.e., length of B define the number of rows, and length of A define the number of columns. dtw_mat = np . zeros (( len ( B ), len ( A ))) print ( dtw_mat ) Output: [[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.]] Now, we need to loop through all element in the dtw matrix, and compute the cumulative distance. # define the absolute distance function d = lambda x , y : np . abs ( x - y ) for i in range ( len ( B )): for j in range ( len ( A )): if i == 0 and j == 0 : dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) else : if i == 0 and j > 0 : choice = dtw_mat [ i , j - 1 ] elif i > 0 and j == 0 : choice = dtw_mat [ i - 1 , j ] else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) + np . min ( choice ) print ( dtw_mat ) Output: [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] We can find the warping path by backtracking. path = [[ len ( B ) - 1 , len ( A ) - 1 ]] while ( True ): print ( path ) i , j = path [ - 1 ][ 0 ], path [ - 1 ][ 1 ] if i == 0 and j == 0 : break elif i == 0 and j > 0 : path . append ([ i , j - 1 ]) elif i > 0 and j == 0 : path . append ([ i - 1 , j ]) else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] ind = [[ i - 1 , j ], [ i , j - 1 ], [ i - 1 , j - 1 ]] k = np . argmin ( choice ) path . append ( ind [ k ]) warp = np . zeros (( len ( B ), len ( A ))) for p in path : warp [ p [ 0 ], p [ 1 ]] = 1 print ( warp ) Output: [[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1.]] Then, we can calculate the normalized distance between time series A and time series B. Assume that all the point shares equal weight, i.e., \\(w_s=1\\) for all \\(P_s\\) , then \\(D(A,B)\\) can be computed as follows: \\[ D(A,B) = \\frac{1}{k} \\sum_{s = 1}^k d(P_s) \\] D = np . sum ( warp * dtw_mat ) / len ( path ) print ( f 'Normalized Distance: { D : 2f } ' ) output: Normalized Distance: 2.111111 Wrap Up the Implementation with function \u2693\ufe0e We can also define a few functions to handle the DTW matrix computation, get the warping path and also compute the normalized distance. def computed_dtwMat ( A , B ): d = lambda x , y : np . abs ( x - y ) for i in range ( len ( B )): for j in range ( len ( A )): if i == 0 and j == 0 : dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) else : if i == 0 and j > 0 : choice = dtw_mat [ i , j - 1 ] elif i > 0 and j == 0 : choice = dtw_mat [ i - 1 , j ] else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) + np . min ( choice ) return dtw_mat We can call the above function by providing time series A and time series B as input arguments, and it will return the dtw_mat . dtw_mat = computed_dtwMat ( A , B ) print ( dtw_mat ) Output: [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] We can define a function to get the warping path. def get_warpingPath ( A , B ): dtw_mat = computed_dtwMat ( A , B ) path = [[ len ( B ) - 1 , len ( A ) - 1 ]] while ( True ): i , j = path [ - 1 ][ 0 ], path [ - 1 ][ 1 ] if i == 0 and j == 0 : break elif i == 0 and j > 0 : path . append ([ i , j - 1 ]) elif i > 0 and j == 0 : path . append ([ i - 1 , j ]) else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] ind = [[ i - 1 , j ], [ i , j - 1 ], [ i - 1 , j - 1 ]] k = np . argmin ( choice ) path . append ( ind [ k ]) warp = np . zeros (( len ( B ), len ( A ))) for p in path : warp [ p [ 0 ], p [ 1 ]] = 1 ``` python path , warp , dtw_matrix = get_warpingPath ( A , B ) print ( warp ) print ( dtw_mat ) Output: [[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1.]] [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] Finally, we can have a function to compute the normalized distance. def normalized_dist ( A , B ): path , warp , dtw_matrix = get_warpingPath ( A , B ) D = np . sum ( warp * dtw_mat ) / len ( path ) return D By calling the function normalized_dist , it will return us the normalized distance between time series A and time series B. D = normalized_dist ( A , B ) print ( f 'Normalized Distance: { D : 2f } ' ) Output: Normalized Distance: 2.111111 Summary \u2693\ufe0e A python script containing all the above functions is provided here You can also refer to the example notebook here on how to apply those functions. There is also a useful dtw-python package provided by Toni dot Giorgino. You can visit his site for more information about dtw-python package. You can also install the package by issuing the following command: pip install dtw-python . DTW is available in Matlab through the Signal Processing Toolbox. For more information about using DTW with Matlab, check out the Matlab documentation about DTW here .","title":"Dynamic Time Warping"},{"location":"deep_learning/timeseries/dtw/#warping-function","text":"DTW uses warping function to find the best alignment between two time series sequences. The objective is to find the path through the grids \\[ P = P_1, P_2, ..., P_s, ..., P_k \\] \\[ P_s = (i, j) \\] which minimizes the total distance between them. DTW aims to learn a warping path that dynamically maps the data points of time series A to data points of time series B. Let \\(A\\) and \\(B\\) be time series A and B, respectively, we can compute the time normalized distance between these two time series: \\[ D(A, B) = \\frac{\\sum_{s=1}^k d(P_s) w_s}{\\sum_{s=1}^k w_s} \\] where \\(d(P_s) = d(v_i, v_j)\\) is the distance between value at \\(i\\) th point \\(v_i\\) and value at \\(j\\) th point \\(v_j\\) , and \\(w_s\\) is the weight coefficient. Considering the Figure above, there are many possible warping paths through the grid. That's being said, to search for an optimum path, i.e., \\[ P_o = arg\\min_P (D(A, B)) \\] can be extremely hard when the grid size is big. Furthermore, if a single point of time series A can map onto a large subsection of time series B, it will lead to an unintuitive alignment. Over the decade, a few constraints have been imposed on the warping function:","title":"Warping Function"},{"location":"deep_learning/timeseries/dtw/#1-monotonicity","text":"Monotonicity ensures that the warping path does not go back in time. Given \\(P_s = (i, j)\\) and \\(P_{s-1} = (i', j')\\) , \\(i \\leq i'\\) and \\(j \\leq j'\\) forces the points in \\(P\\) to be monotonically spaced in time","title":"1. Monotonicity"},{"location":"deep_learning/timeseries/dtw/#2-continuity","text":"Continuity ensures that the warping path does not jump in time. Given \\(P_s = (i, j)\\) and \\(P_{s-1} = (i', j')\\) , \\(i - i' \\leq 1\\) and \\(j - j' \\leq 1\\) restricts the allowable steps in the warping path to adjacent cells.","title":"2. Continuity"},{"location":"deep_learning/timeseries/dtw/#3-boundary-conditions","text":"The boundary of the warping path states that the part should start at \\(P_1 = (1, 1)\\) and end at \\(P_s = (m, n)\\) , i.e., the warping path needs to start and finish in diagonally opposite corner of the grid. This is important to make sure the warping path does not consider only partial of the sequence.","title":"3. Boundary Conditions"},{"location":"deep_learning/timeseries/dtw/#4-warping-window","text":"Let \\(r>0\\) be the length of warping window, \\(\\|i - j\\| \\leq r\\) restricts allowable grid points for the warping path. The warping window ensures that the warping path does not wander too far away from the diagonal. This guarantees that the alignment will not get stuck at similar features.","title":"4. Warping Window"},{"location":"deep_learning/timeseries/dtw/#5-slope-constraint","text":"Slope constraint ensures that the warping path is neither too steep or too shallow. Let \\(q\\) and \\(p\\) be the number of steps in the x-direction and y-direction given the grid, then \\(\\frac{j_{p} - j_{0}}{i_{p} - i_{0}} \\leq p\\) and \\(\\frac{i_{q} - i_{0}}{j_{q} - j_{0}} \\leq q\\) . That is, after \\(q\\) steps in x one must step in y and vice versa.","title":"5. Slope Constraint"},{"location":"deep_learning/timeseries/dtw/#dynamic-programming","text":"Dynamic Programming is an efficient method to find the warping path. In general, dynamic programming evaluate the cumulative distance \\(\\gamma(i, j)\\) based on the distance \\(d(P_s)\\) at the current cell and the minimum of the cumulative distances of the adjacent elements. Mathematically, \\[ \\gamma(i, j) = d(P_s) + \\min \\{\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\} \\] where \\(d(P_s) = d(v_i, v_j)\\) can be calculated by taking the absolute difference between value at \\(i\\) th point and value at \\(j\\) th point, i.e., \\(d(v_i, v_j) = |v_i - v_j|\\)","title":"Dynamic Programming"},{"location":"deep_learning/timeseries/dtw/#example","text":"Suppose that we have two time series A and B as follows: \\[ A = [ 3, 2, 2, 3, 5, 5, 6 ] \\] \\[ B = [ 1, 3, 2, 2, 3, 5 ] \\] First, let's consider point \\(P_s = (1,1)\\) , i.e., \\(i=1\\) and \\(j=1\\) , the value at \\(i=1\\) is \\(v(i=1) = 3\\) and the value at \\(j=1\\) is \\(v(j=1) = 1\\) . Hence, the absolute distance at point \\(P_s = (1,1)\\) is \\(d(P_s) = |3-1| = 2\\) . Since the values at the cell above, left and diagonally above \\(P_s\\) are all empty, hence, \\(\\min \\{\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\} = 0\\) . The cumulative distance \\(\\gamma(i=1, j=1)\\) will be 2, as shown in Figure below. Let's consider another point \\(P_s = (4,4)\\) , here we have \\(v(i=4) = 3\\) and \\(v(j=4) = 2\\) . Hence, the absolute distance at point \\(P_s = (4,4)\\) is \\(d(P_s) = |3-2| = 1\\) . The values at the adjacent cells (above, left and diagonally above ) are 3, 2 and 2. By taking the minimum value, we have 2. So, the cumulative distance \\(\\gamma(i=4, j=4)\\) will be 3, as shown in Figure below. Figure below shows the final grid (or matrix) with all the value computed. The warping path can be obtained by backtracking from the end point to the beginning point as shown below.","title":"Example"},{"location":"deep_learning/timeseries/dtw/#implementation","text":"Here we will walkthrough a simple implementation of above example in Python. First, define the two time series A and B A = np . array ([ 3 , 2 , 2 , 3 , 5 , 5 , 6 ]) B = np . array ([ 1 , 3 , 2 , 2 , 3 , 5 ]) Initialize a dtw matrix based on the length of A and B, i.e., length of B define the number of rows, and length of A define the number of columns. dtw_mat = np . zeros (( len ( B ), len ( A ))) print ( dtw_mat ) Output: [[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0.]] Now, we need to loop through all element in the dtw matrix, and compute the cumulative distance. # define the absolute distance function d = lambda x , y : np . abs ( x - y ) for i in range ( len ( B )): for j in range ( len ( A )): if i == 0 and j == 0 : dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) else : if i == 0 and j > 0 : choice = dtw_mat [ i , j - 1 ] elif i > 0 and j == 0 : choice = dtw_mat [ i - 1 , j ] else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) + np . min ( choice ) print ( dtw_mat ) Output: [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] We can find the warping path by backtracking. path = [[ len ( B ) - 1 , len ( A ) - 1 ]] while ( True ): print ( path ) i , j = path [ - 1 ][ 0 ], path [ - 1 ][ 1 ] if i == 0 and j == 0 : break elif i == 0 and j > 0 : path . append ([ i , j - 1 ]) elif i > 0 and j == 0 : path . append ([ i - 1 , j ]) else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] ind = [[ i - 1 , j ], [ i , j - 1 ], [ i - 1 , j - 1 ]] k = np . argmin ( choice ) path . append ( ind [ k ]) warp = np . zeros (( len ( B ), len ( A ))) for p in path : warp [ p [ 0 ], p [ 1 ]] = 1 print ( warp ) Output: [[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1.]] Then, we can calculate the normalized distance between time series A and time series B. Assume that all the point shares equal weight, i.e., \\(w_s=1\\) for all \\(P_s\\) , then \\(D(A,B)\\) can be computed as follows: \\[ D(A,B) = \\frac{1}{k} \\sum_{s = 1}^k d(P_s) \\] D = np . sum ( warp * dtw_mat ) / len ( path ) print ( f 'Normalized Distance: { D : 2f } ' ) output: Normalized Distance: 2.111111","title":"Implementation"},{"location":"deep_learning/timeseries/dtw/#wrap-up-the-implementation-with-function","text":"We can also define a few functions to handle the DTW matrix computation, get the warping path and also compute the normalized distance. def computed_dtwMat ( A , B ): d = lambda x , y : np . abs ( x - y ) for i in range ( len ( B )): for j in range ( len ( A )): if i == 0 and j == 0 : dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) else : if i == 0 and j > 0 : choice = dtw_mat [ i , j - 1 ] elif i > 0 and j == 0 : choice = dtw_mat [ i - 1 , j ] else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] dtw_mat [ i , j ] = d ( B [ i ], A [ j ]) + np . min ( choice ) return dtw_mat We can call the above function by providing time series A and time series B as input arguments, and it will return the dtw_mat . dtw_mat = computed_dtwMat ( A , B ) print ( dtw_mat ) Output: [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] We can define a function to get the warping path. def get_warpingPath ( A , B ): dtw_mat = computed_dtwMat ( A , B ) path = [[ len ( B ) - 1 , len ( A ) - 1 ]] while ( True ): i , j = path [ - 1 ][ 0 ], path [ - 1 ][ 1 ] if i == 0 and j == 0 : break elif i == 0 and j > 0 : path . append ([ i , j - 1 ]) elif i > 0 and j == 0 : path . append ([ i - 1 , j ]) else : choice = [ dtw_mat [ i - 1 , j ], dtw_mat [ i , j - 1 ], dtw_mat [ i - 1 , j - 1 ]] ind = [[ i - 1 , j ], [ i , j - 1 ], [ i - 1 , j - 1 ]] k = np . argmin ( choice ) path . append ( ind [ k ]) warp = np . zeros (( len ( B ), len ( A ))) for p in path : warp [ p [ 0 ], p [ 1 ]] = 1 ``` python path , warp , dtw_matrix = get_warpingPath ( A , B ) print ( warp ) print ( dtw_mat ) Output: [[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1.]] [[ 2. 3. 4. 6. 10. 14. 19.] [ 2. 3. 4. 4. 6. 8. 11.] [ 3. 2. 2. 3. 6. 9. 12.] [ 4. 2. 2. 3. 6. 9. 13.] [ 4. 3. 3. 2. 4. 6. 9.] [ 6. 6. 6. 4. 2. 2. 3.]] Finally, we can have a function to compute the normalized distance. def normalized_dist ( A , B ): path , warp , dtw_matrix = get_warpingPath ( A , B ) D = np . sum ( warp * dtw_mat ) / len ( path ) return D By calling the function normalized_dist , it will return us the normalized distance between time series A and time series B. D = normalized_dist ( A , B ) print ( f 'Normalized Distance: { D : 2f } ' ) Output: Normalized Distance: 2.111111","title":"Wrap Up the Implementation with function"},{"location":"deep_learning/timeseries/dtw/#summary","text":"A python script containing all the above functions is provided here You can also refer to the example notebook here on how to apply those functions. There is also a useful dtw-python package provided by Toni dot Giorgino. You can visit his site for more information about dtw-python package. You can also install the package by issuing the following command: pip install dtw-python . DTW is available in Matlab through the Signal Processing Toolbox. For more information about using DTW with Matlab, check out the Matlab documentation about DTW here .","title":"Summary"},{"location":"programming/matlab/","text":"--","title":"Matlab"},{"location":"programming/python/2021-04-18-flattenRavel/","text":"Dealing with multidimensional array with Numpy is quite intuitive. While doing an element-wise multipilication computation with Numpy is basically more easier compared to using a classical programming with a for loop, understanding the internal operation of Numpy is useful to improve the computation efficiency when dealing with arrays consists of millions of elements. In this post, we will look at the differences between flatten and ravel functions. Both functions provide the same one-dimensional output by stacking up a multidimensional inputs. The key difference is how the memory is copied during the process. Let's say we would like to flatten an 1000*1000 dimensional array, using flatten will returns a copy, whereas using ravel will returns a view. The computation time of both functions is shown as follows: import numpy as np # create a 1000*1000 dimensional array arr = np . random . rand ( 1000 , 1000 ) print ( f 'Size of arr: { arr . shape } ' ) Size of arr: (1000, 1000) %% time arr_flatten = arr . flatten () Wall time: 3.95 ms %% time arr_ravel = arr . ravel () Wall time: 0 ns Obviously, ravel is much more faster than flatten . Such a performance speedup can be significant when leading with very large arrays. We can also check that both ravel and flatten functions returns the same output. print ( np . array_equal ( arr_flatten , arr_ravel )) True The difference is that there is not copy operation with ravel . For flatten , the output is a copy of the original array; wheareas for ravel , the output is just a view of original array, in which whatever the changes in the second array will affect the change in the original array. To understand the difference between a copy and a view, consider the memory block of these three arrays. print ( f 'Memory address to store arr: { arr . __array_interface__ [ \"data\" ][ 0 ] } ' ) print ( f 'Memory address to store arr_flatten: { arr_flatten . __array_interface__ [ \"data\" ][ 0 ] } ' ) print ( f 'Memory address to store arr_ravel: { arr_ravel . __array_interface__ [ \"data\" ][ 0 ] } ' ) Memory address to store arr: 2394595090496 Memory address to store arr_flatten: 2394603155520 Memory address to store arr_ravel: 2394595090496 We can see that flatten uses copy the array to a new memory block; whreas ravel simply creates a view to the original array. When array is not in C-order \u2693\ufe0e Note that ravel will also do a copy operation when dealing with array that is not in the C-order. For example, when we consider the array in Fortrain-order, as in a.T , ravel actually returns a flattened version with C-order. %% time arr_ravel2 = arr . ravel () Wall time: 0 ns %% time arr_ravel2T = arr . T . ravel () Wall time: 7.07 ms When dealing with the array in different order, we can specify the order with ravel isntead. %% time arr_ravel3T = arr . ravel ( order = 'F' ) Wall time: 6.03 ms By specifying the order directly within the ravel function, the computation time is slightly faster than doing ravel directly on arr.T .","title":"Flatten vs Ravel functions in Numpy"},{"location":"programming/python/2021-04-18-flattenRavel/#when-array-is-not-in-c-order","text":"Note that ravel will also do a copy operation when dealing with array that is not in the C-order. For example, when we consider the array in Fortrain-order, as in a.T , ravel actually returns a flattened version with C-order. %% time arr_ravel2 = arr . ravel () Wall time: 0 ns %% time arr_ravel2T = arr . T . ravel () Wall time: 7.07 ms When dealing with the array in different order, we can specify the order with ravel isntead. %% time arr_ravel3T = arr . ravel ( order = 'F' ) Wall time: 6.03 ms By specifying the order directly within the ravel function, the computation time is slightly faster than doing ravel directly on arr.T .","title":"When array is not in C-order"},{"location":"programming/python/histogramVis/","text":"Histogram allows us to visualize the frequency distribution of our data. It breaks the data into a few smaller bins according to the value of the data, and then count the number of occurences (i.e., the frequency) in each bin. We can obtain the frequency and bins for a given data using the histogram() function from numpy. Let's consider the following example: import numpy as np # generate 1000 random numbers x = np . random . rand ( 1000 , 1 ) # count the occurences in each bin in x frequency , bins = np . histogram ( x , bins = 10 , range = [ 0 , 1 ]) for b , f in zip ( bins [ 1 :], frequency ): print ( f 'value: { ( round ( b , 1 )) } >> frequency: { f } ' ) Output: value: 0.1 >> frequency: 80 value: 0.2 >> frequency: 80 value: 0.3 >> frequency: 98 value: 0.4 >> frequency: 91 value: 0.5 >> frequency: 123 value: 0.6 >> frequency: 97 value: 0.7 >> frequency: 105 value: 0.8 >> frequency: 102 value: 0.9 >> frequency: 117 value: 1.0 >> frequency: 107 Here, we used numpy.random.rand() function to generate 1000 uniformly distributed values, ranging from 0 to 1. An array x is defined to store the generated values. We would like to know how many data is within 0-0.1, how many occurs at 0.1-0.2, and so on. These can be obtained by calling numpy.histogram() function. Histogram with Matplotlib \u2693\ufe0e Matplotlib allows us to plot the histogram with pyplot.hist() function. Let's continue with the above example, and use the histogram function in Matplotlib to visualize the data distribution. import matplotlib.pyplot as plt plt . hist ( x , bins = 10 ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . show () svg Let's generate another set of random number, but with normal distribution. Instead of numpy.random.rand() , we can use numpy.random.randn() to generate a series of values that follow standard normal distribution with zero mean and standard deviation equals to 1. Notes: if we would like to have a normal distribution with specific mean and standard deviation, we can use the following formula: $$ \\sigma * numpy.random.randn() + \\mu $$ # generate 1000 random numbers x = np . random . randn ( 1000 , 1 ) plt . hist ( x , bins = 10 ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . show () svg Multiple Histograms in a Single Plot \u2693\ufe0e We can plot multiple histograms for easy comparison. Let's create 3 numpy arrays each consists of 1000 normally distributed random numbers based on different mean and standard deviation. x1 = 3 * np . random . randn ( 1000 , 1 ) + 3 x2 = 2 * np . random . randn ( 1000 , 1 ) + 7 x3 = x plt . figure ( figsize = ( 12 , 5 )) plt . hist ( x1 , bins = 10 , alpha = 0.5 , color = 'red' , label = 'x1' ) plt . hist ( x2 , bins = 10 , alpha = 0.5 , color = 'green' , label = 'x2' ) plt . hist ( x3 , bins = 10 , alpha = 0.5 , color = 'blue' , label = 'x3' ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . legend () plt . show () svg Density Distribution \u2693\ufe0e Instead of using the number of occurences as the y-axis, we can normalize the occurences frequency by setting density to True , as shown below: plt . figure () plt . hist ( x , bins = 10 , density = True ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg Style the Histogram \u2693\ufe0e We can create spacing between each bin in the histogram using the set_style() function from seaborn. Note that seaborn is built upon matplotlib, so we can use seaborn and matplotlib together. import seaborn as sns sns . set_style ( \"white\" ) plt . figure () plt . hist ( x , bins = 10 , density = True ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg We can also use the seaborn.histplot() function to visualize the histogram and density curve on the same plot. plt . figure () # sns.histplot(x, bins = 10, hist_kws={'alpha': 0.5}, kde_kws={'linewidth': 2}) sns . histplot ( x , bins = 10 , alpha = 0.5 , stat = \"probability\" , kde = True , legend = False ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg","title":"Visualize Data Distribution with Histogram"},{"location":"programming/python/histogramVis/#histogram-with-matplotlib","text":"Matplotlib allows us to plot the histogram with pyplot.hist() function. Let's continue with the above example, and use the histogram function in Matplotlib to visualize the data distribution. import matplotlib.pyplot as plt plt . hist ( x , bins = 10 ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . show () svg Let's generate another set of random number, but with normal distribution. Instead of numpy.random.rand() , we can use numpy.random.randn() to generate a series of values that follow standard normal distribution with zero mean and standard deviation equals to 1. Notes: if we would like to have a normal distribution with specific mean and standard deviation, we can use the following formula: $$ \\sigma * numpy.random.randn() + \\mu $$ # generate 1000 random numbers x = np . random . randn ( 1000 , 1 ) plt . hist ( x , bins = 10 ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . show () svg","title":"Histogram with Matplotlib"},{"location":"programming/python/histogramVis/#multiple-histograms-in-a-single-plot","text":"We can plot multiple histograms for easy comparison. Let's create 3 numpy arrays each consists of 1000 normally distributed random numbers based on different mean and standard deviation. x1 = 3 * np . random . randn ( 1000 , 1 ) + 3 x2 = 2 * np . random . randn ( 1000 , 1 ) + 7 x3 = x plt . figure ( figsize = ( 12 , 5 )) plt . hist ( x1 , bins = 10 , alpha = 0.5 , color = 'red' , label = 'x1' ) plt . hist ( x2 , bins = 10 , alpha = 0.5 , color = 'green' , label = 'x2' ) plt . hist ( x3 , bins = 10 , alpha = 0.5 , color = 'blue' , label = 'x3' ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Frequency\" ) plt . legend () plt . show () svg","title":"Multiple Histograms in a Single Plot"},{"location":"programming/python/histogramVis/#density-distribution","text":"Instead of using the number of occurences as the y-axis, we can normalize the occurences frequency by setting density to True , as shown below: plt . figure () plt . hist ( x , bins = 10 , density = True ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg","title":"Density Distribution"},{"location":"programming/python/histogramVis/#style-the-histogram","text":"We can create spacing between each bin in the histogram using the set_style() function from seaborn. Note that seaborn is built upon matplotlib, so we can use seaborn and matplotlib together. import seaborn as sns sns . set_style ( \"white\" ) plt . figure () plt . hist ( x , bins = 10 , density = True ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg We can also use the seaborn.histplot() function to visualize the histogram and density curve on the same plot. plt . figure () # sns.histplot(x, bins = 10, hist_kws={'alpha': 0.5}, kde_kws={'linewidth': 2}) sns . histplot ( x , bins = 10 , alpha = 0.5 , stat = \"probability\" , kde = True , legend = False ) plt . title ( \"Data Distribution\" ) plt . xlabel ( \"Value\" ) plt . ylabel ( \"Probability\" ) plt . show () svg","title":"Style the Histogram"},{"location":"tags/","text":"Tags \u2693\ufe0e","title":"Tags"},{"location":"tags/#tags","text":"","title":"Tags"}]}